{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQmXi86x5ZF6uk3eccpUKy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TsRyVD7rAQ6","executionInfo":{"status":"ok","timestamp":1761082101550,"user_tz":420,"elapsed":17383,"user":{"displayName":"Lea Teigelkotter","userId":"17764228621487427622"}},"outputId":"9029295b-4c22-42cb-b7e4-b6a9ee9b1c60"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbIJUWEHqMdh","executionInfo":{"status":"ok","timestamp":1761082451834,"user_tz":420,"elapsed":348263,"user":{"displayName":"Lea Teigelkotter","userId":"17764228621487427622"}},"outputId":"db597388-aa92-44be-d13c-d74fa1996ce5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing: us_elections_2017_2018_strict.csv\n","Processing: us_elections_2020_strict.csv\n","Finalizing…\n","Wrote -> /content/drive/MyDrive/nyt_outputs/article_level_targets_textblob.csv rows: 950\n"]}],"source":["# ==== Per-article targets with TextBlob (chunked, low-memory) ====\n","# Outputs: /content/drive/MyDrive/nyt_outputs/article_level_targets_textblob.csv\n","\n","!pip -q install textblob\n","\n","import os, math, gc\n","import pandas as pd\n","from collections import defaultdict\n","from textblob import TextBlob\n","\n","# ---------- CONFIG ----------\n","DRIVE_DIR = \"/content/drive/MyDrive/nyt_outputs\"\n","COMBINED_STRICT = os.path.join(DRIVE_DIR, \"us_elections_combined_strict.csv\")\n","STRICT_1718    = os.path.join(DRIVE_DIR, \"us_elections_2017_2018_strict.csv\")\n","STRICT_2020    = os.path.join(DRIVE_DIR, \"us_elections_2020_strict.csv\")\n","OUT_CSV        = os.path.join(DRIVE_DIR, \"article_level_targets_textblob.csv\")\n","\n","# Use combined if present; else fall back to per-year strict files\n","INPUTS = [COMBINED_STRICT] if os.path.exists(COMBINED_STRICT) else [p for p in (STRICT_1718, STRICT_2020) if os.path.exists(p)]\n","assert INPUTS, \"No strict CSVs found in Drive. Copy *_strict.csv files to /content/drive/MyDrive/nyt_outputs first.\"\n","\n","# Columns we will read from the strict CSVs\n","META_COLS = [\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\"abstract\",\"news_desk\",\"type_of_material\",\"keywords\"]\n","NEEDED_COLS = [\"article_id\",\"comment\"] + META_COLS\n","\n","# ---------- helpers ----------\n","def safe_polarity(text):\n","    if text is None or (isinstance(text, float) and math.isnan(text)):\n","        return 0.0\n","    s = str(text).strip()\n","    if not s:\n","        return 0.0\n","    try:\n","        return TextBlob(s).sentiment.polarity  # [-1,1]\n","    except Exception:\n","        return 0.0\n","\n","# Aggregation holders\n","# numeric: sum of polarity and count of comments\n","agg_sum = defaultdict(float)   # article_id -> sum polarity\n","agg_cnt = defaultdict(int)     # article_id -> num comments\n","# metadata: keep first non-null we see\n","agg_meta = {}  # article_id -> dict of META_COLS\n","\n","# ---------- main loop (chunked) ----------\n","for path in INPUTS:\n","    print(f\"Processing: {os.path.basename(path)}\")\n","    for i, chunk in enumerate(pd.read_csv(path, low_memory=False, chunksize=100_000, usecols=lambda c: c in NEEDED_COLS, dtype={\"article_id\": str})):\n","        # sentiment per comment\n","        chunk[\"comment\"] = chunk[\"comment\"].astype(str)\n","        chunk[\"polarity\"] = chunk[\"comment\"].map(safe_polarity)\n","\n","        # numeric aggregation per chunk\n","        sums  = chunk.groupby(\"article_id\")[\"polarity\"].sum()\n","        counts= chunk.groupby(\"article_id\")[\"polarity\"].count()\n","\n","        for aid, s in sums.items():\n","            agg_sum[aid] += float(s)\n","        for aid, n in counts.items():\n","            agg_cnt[aid] += int(n)\n","\n","        # metadata: take first occurrence per article_id if not already stored\n","        meta_first = chunk.drop_duplicates(\"article_id\")[[\"article_id\"] + META_COLS].set_index(\"article_id\")\n","        for aid, row in meta_first.iterrows():\n","            if aid not in agg_meta:\n","                agg_meta[aid] = {k: row.get(k) for k in META_COLS}\n","\n","        if (i+1) % 20 == 0:\n","            print(f\"  chunks processed: {i+1}, distinct articles so far: {len(agg_meta):,}\")\n","        del chunk, sums, counts, meta_first\n","        gc.collect()\n","\n","print(\"Finalizing…\")\n","\n","# ---------- build output frame ----------\n","rows = []\n","for aid in agg_cnt.keys():\n","    total = agg_cnt[aid]\n","    sent_mean = (agg_sum[aid] / total) if total else 0.0\n","    meta = agg_meta.get(aid, {})\n","    rows.append({\n","        \"article_id\": aid,\n","        **{k: meta.get(k) for k in META_COLS},\n","        \"comments_total\": total,\n","        \"sentiment_mean_textblob\": sent_mean,\n","    })\n","\n","out_df = pd.DataFrame(rows)\n","\n","# Optional: sort by pub_date then comments_total desc\n","# (pub_date might be string; we won't coerce here to keep it simple/fast)\n","if \"pub_date\" in out_df.columns:\n","    out_df = out_df.sort_values(by=[\"pub_date\",\"comments_total\"], ascending=[True, False], ignore_index=True)\n","\n","# Write to Drive\n","out_df.to_csv(OUT_CSV, index=False)\n","print(\"Wrote ->\", OUT_CSV, \"rows:\", len(out_df))\n"]}]}