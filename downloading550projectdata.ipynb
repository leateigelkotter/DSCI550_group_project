{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fw9uNMTzCq1",
        "outputId": "dac8ec4f-e5c3-4bfc-9230-6aeb9bead77e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NYT articles+comments join & filter (inline version) ---\n",
        "import os, re, glob\n",
        "from typing import List, Tuple, Optional\n",
        "import pandas as pd\n",
        "\n",
        "def _clean_url(u):\n",
        "    if pd.isna(u): return u\n",
        "    u = str(u).strip()\n",
        "    return u.split('?')[0].rstrip('/').lower()\n",
        "\n",
        "def _parse_keywords(val):\n",
        "    if pd.isna(val): return []\n",
        "    if isinstance(val, list): return [str(x).strip() for x in val if str(x).strip()]\n",
        "    return [p.strip() for p in re.split(r'[;,]\\s*', str(val)) if p.strip()]\n",
        "\n",
        "ARTICLE_COLS = [\"article_id\",\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\"abstract\",\"news_desk\",\"type_of_material\",\"keywords\"]\n",
        "COMMENT_COLS = [\"comment_id\",\"parent_id\",\"user\",\"comment\",\"recommendations\",\"reply_count\",\"editors_selection\",\"date\",\"article_url\",\"article_id\"]\n",
        "\n",
        "def normalize_articles(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    cand = {\n",
        "        \"article_id\":[\"articleID\",\"asset_id\",\"assetId\",\"id\",\"article_id\"],\n",
        "        \"url\":[\"articleURL\",\"url\",\"web_url\",\"link\"],\n",
        "        \"pub_date\":[\"pub_date\",\"pubDate\",\"date\",\"published_date\",\"PublicationDate\"],\n",
        "        \"section\":[\"section\",\"section_name\",\"newsSection\",\"news_desk\"],\n",
        "        \"subsection\":[\"subsection\",\"subsection_name\",\"sub_section\"],\n",
        "        \"headline\":[\"headline\",\"title\",\"main_headline\",\"headline.main\",\"Title\"],\n",
        "        \"abstract\":[\"abstract\",\"snippet\",\"abstract_text\",\"lead_paragraph\",\"Summary\"],\n",
        "        \"news_desk\":[\"news_desk\",\"desk\"],\n",
        "        \"type_of_material\":[\"type_of_material\",\"type\",\"material_type_facet\"],\n",
        "        \"keywords\":[\"keywords\",\"descriptors\",\"subject\",\"keywords_list\"],\n",
        "    }\n",
        "    for canon, opts in cand.items():\n",
        "        for o in opts:\n",
        "            if o in df.columns:\n",
        "                df[canon] = df[o]; break\n",
        "    if \"url\" in df: df[\"url\"] = df[\"url\"].map(_clean_url)\n",
        "    if \"keywords\" in df: df[\"keywords\"] = df[\"keywords\"].map(_parse_keywords)\n",
        "    else: df[\"keywords\"] = [[] for _ in range(len(df))]\n",
        "    for c in ARTICLE_COLS:\n",
        "        if c not in df: df[c] = pd.NA\n",
        "    return df[ARTICLE_COLS]\n",
        "\n",
        "def normalize_comments(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    cand = {\n",
        "        \"comment_id\":[\"commentID\",\"comment_id\",\"id\"],\n",
        "        \"parent_id\":[\"parentID\",\"parent_id\",\"inReplyTo\"],\n",
        "        \"user\":[\"userDisplayName\",\"user\",\"display_name\",\"author\"],\n",
        "        \"comment\":[\"commentBody\",\"comment\",\"text\",\"body\"],\n",
        "        \"recommendations\":[\"recommendations\",\"recommendCount\",\"recommendedCount\"],\n",
        "        \"reply_count\":[\"replyCount\",\"replies\",\"numReplies\"],\n",
        "        \"editors_selection\":[\"editorsSelection\",\"editors_selection\"],\n",
        "        \"date\":[\"createDate\",\"date\",\"timestamp\"],\n",
        "        \"article_url\":[\"articleURL\",\"url\",\"articleUrl\",\"web_url\"],\n",
        "        \"article_id\":[\"articleID\",\"asset_id\",\"assetId\",\"article_id\"],\n",
        "    }\n",
        "    for canon, opts in cand.items():\n",
        "        for o in opts:\n",
        "            if o in df.columns:\n",
        "                df[canon] = df[o]; break\n",
        "    if \"article_url\" in df: df[\"article_url\"] = df[\"article_url\"].map(_clean_url)\n",
        "    for c in COMMENT_COLS:\n",
        "        if c not in df: df[c] = pd.NA\n",
        "    return df[COMMENT_COLS]\n",
        "\n",
        "def load_2017_2018(base_dir: str):\n",
        "    a_paths = sorted(glob.glob(os.path.join(base_dir, \"Articles*.csv\")))\n",
        "    c_paths = sorted(glob.glob(os.path.join(base_dir, \"Comments*.csv\")))\n",
        "    if not a_paths: raise FileNotFoundError(\"No Articles*.csv in \" + base_dir)\n",
        "    if not c_paths: raise FileNotFoundError(\"No Comments*.csv in \" + base_dir)\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in a_paths], ignore_index=True)\n",
        "    comms = pd.concat([pd.read_csv(p, low_memory=False) for p in c_paths], ignore_index=True)\n",
        "    return normalize_articles(arts), normalize_comments(comms)\n",
        "\n",
        "def load_2020(base_dir: str):\n",
        "    a_paths = sorted(set(glob.glob(os.path.join(base_dir,\"*articles*.csv\")) + glob.glob(os.path.join(base_dir,\"articles*.csv\"))))\n",
        "    c_paths = sorted(set(glob.glob(os.path.join(base_dir,\"*comments*.csv\")) + glob.glob(os.path.join(base_dir,\"comments*.csv\"))))\n",
        "    if not a_paths: raise FileNotFoundError(\"No *articles*.csv in \" + base_dir)\n",
        "    if not c_paths: raise FileNotFoundError(\"No *comments*.csv in \" + base_dir)\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in a_paths], ignore_index=True)\n",
        "    comms = pd.concat([pd.read_csv(p, low_memory=False) for p in c_paths], ignore_index=True)\n",
        "    return normalize_articles(arts), normalize_comments(comms)\n",
        "\n",
        "def join_articles_comments(articles: pd.DataFrame, comments: pd.DataFrame) -> pd.DataFrame:\n",
        "    a, c = articles.copy(), comments.copy()\n",
        "    can_id = a[\"article_id\"].notna().any() and c[\"article_id\"].notna().any()\n",
        "    if can_id:\n",
        "        m = c.merge(a, how=\"left\", on=\"article_id\", suffixes=(\"_c\",\"_a\"))\n",
        "        missing = m[\"headline\"].isna()\n",
        "        if missing.any():\n",
        "            fill = c.loc[missing, [\"article_url\"]].merge(\n",
        "                a[[\"url\",\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"article_id\"]],\n",
        "                how=\"left\", left_on=\"article_url\", right_on=\"url\"\n",
        "            )\n",
        "            m.loc[missing, [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"article_id\"]] = \\\n",
        "                fill[[\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"article_id\"]].values\n",
        "        return m\n",
        "    return c.merge(a, how=\"left\", left_on=\"article_url\", right_on=\"url\", suffixes=(\"_c\",\"_a\"))\n",
        "\n",
        "SECTION_ALLOW = {\"u.s.\",\"us\",\"us news\",\"politics\",\"business\",\"economy\"}\n",
        "KEYWORD_PATTERNS = [\n",
        "    r\"\\bpolitic(s|al)?\\b\", r\"\\bcongress\\b\", r\"\\bsenate\\b\", r\"\\bhouse of representatives\\b\",\n",
        "    r\"\\bwhite house\\b\", r\"\\bpresident\\b\", r\"\\bgovern(or|ment)\\b\", r\"\\bgubernatorial\\b\",\n",
        "    r\"\\belection(s)?\\b\", r\"\\bprimary\\b\", r\"\\bmidterm(s)?\\b\", r\"\\bballot\\b\", r\"\\bcampaign\\b\",\n",
        "    r\"\\bsupreme court\\b\", r\"\\bscotus\\b\", r\"\\bjustice department\\b\", r\"\\battorney general\\b\",\n",
        "    r\"\\bimpeachment\\b\", r\"\\blobby(ist|ing)\\b\", r\"\\bpublic policy\\b\",\n",
        "    r\"\\beconom(y|ic|ics)\\b\", r\"\\bgdp\\b\", r\"\\binflation\\b\", r\"\\bunemployment\\b\",\n",
        "    r\"\\bjobs? report\\b\", r\"\\bfederal reserve\\b\", r\"\\binterest rate(s)?\\b\",\n",
        "    r\"\\bstimulus\\b\", r\"\\btrade\\b\", r\"\\btariff(s)?\\b\", r\"\\bmanufactur(ing|ers?)\\b\",\n",
        "    r\"\\bconsumer confidence\\b\", r\"\\bretail sales\\b\", r\"\\blabor market\\b\", r\"\\bdeficit\\b\", r\"\\bnational debt\\b\",\n",
        "]\n",
        "KEYWORD_REGEX = re.compile(\"|\".join(KEYWORD_PATTERNS), re.IGNORECASE)\n",
        "\n",
        "def _is_domestic(row):\n",
        "    sec = str(row.get(\"section\") or \"\").strip().lower()\n",
        "    if sec in SECTION_ALLOW: return True\n",
        "    sub = str(row.get(\"subsection\") or \"\").strip().lower()\n",
        "    if any(x in sub for x in [\"politic\",\"u.s.\",\"business\",\"economy\"]): return True\n",
        "    kws = row.get(\"keywords\", [])\n",
        "    if isinstance(kws, list):\n",
        "        if any(KEYWORD_REGEX.search(str(k)) for k in kws): return True\n",
        "    else:\n",
        "        if KEYWORD_REGEX.search(str(kws)): return True\n",
        "    for col in (\"headline\",\"abstract\"):\n",
        "        if KEYWORD_REGEX.search(str(row.get(col) or \"\")): return True\n",
        "    return False\n",
        "\n",
        "def filter_domestic_politics_econ(merged: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = merged.copy()\n",
        "    mask = m.apply(_is_domestic, axis=1)\n",
        "    return m[mask].reset_index(drop=True)\n",
        "\n",
        "def build_dataset(path_2017_2018: str, path_2020: str, out_dir: Optional[str]=None, write_csv: bool=False):\n",
        "    artsA, commsA = load_2017_2018(path_2017_2018)\n",
        "    artsB, commsB = load_2020(path_2020)\n",
        "    mergedA = join_articles_comments(artsA, commsA)\n",
        "    mergedB = join_articles_comments(artsB, commsB)\n",
        "    combined = pd.concat([filter_domestic_politics_econ(mergedA), filter_domestic_politics_econ(mergedB)], ignore_index=True)\n",
        "    if write_csv and out_dir:\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        mergedA.to_csv(os.path.join(out_dir,\"joined_2017_2018_all.csv\"), index=False)\n",
        "        mergedB.to_csv(os.path.join(out_dir,\"joined_2020_all.csv\"), index=False)\n",
        "        combined.to_csv(os.path.join(out_dir,\"domestic_politics_econ_filtered.csv\"), index=False)\n",
        "    return mergedA, mergedB, combined\n",
        "# --- end inline module ---\n"
      ],
      "metadata": {
        "id": "c27FeSPDzst3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Chunked NYT join+filter for Colab (memory-safe) ---\n",
        "import os, re, glob, math\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "\n",
        "# --------- helpers ----------\n",
        "def _clean_url(u):\n",
        "    if pd.isna(u): return u\n",
        "    return str(u).strip().split('?')[0].rstrip('/').lower()\n",
        "\n",
        "def _parse_keywords(val):\n",
        "    if pd.isna(val): return []\n",
        "    if isinstance(val, list): return [str(x).strip() for x in val if str(x).strip()]\n",
        "    return [p.strip() for p in re.split(r'[;,]\\s*', str(val)) if p.strip()]\n",
        "\n",
        "def _to_lower_str(val):\n",
        "    # Robustly convert to a safe lowercased string (handles pd.NA/NaN/None)\n",
        "    if val is None: return \"\"\n",
        "    if val is pd.NA: return \"\"\n",
        "    try:\n",
        "        if isinstance(val, float) and math.isnan(val):\n",
        "            return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    if hasattr(pd, \"isna\") and pd.isna(val):\n",
        "        return \"\"\n",
        "    return str(val).strip().lower()\n",
        "\n",
        "ARTICLE_COLS = [\n",
        "    \"article_id\",\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\n",
        "    \"abstract\",\"news_desk\",\"type_of_material\",\"keywords\"\n",
        "]\n",
        "COMMENT_COLS = [\n",
        "    \"comment_id\",\"parent_id\",\"user\",\"comment\",\"recommendations\",\"reply_count\",\n",
        "    \"editors_selection\",\"date\",\"article_url\",\"article_id\"\n",
        "]\n",
        "\n",
        "def normalize_articles(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    cand = {\n",
        "        \"article_id\":[\"articleID\",\"asset_id\",\"assetId\",\"id\",\"article_id\"],\n",
        "        \"url\":[\"articleURL\",\"url\",\"web_url\",\"link\"],\n",
        "        \"pub_date\":[\"pub_date\",\"pubDate\",\"date\",\"published_date\",\"PublicationDate\"],\n",
        "        \"section\":[\"section\",\"section_name\",\"newsSection\",\"news_desk\"],\n",
        "        \"subsection\":[\"subsection\",\"subsection_name\",\"sub_section\"],\n",
        "        \"headline\":[\"headline\",\"title\",\"main_headline\",\"headline.main\",\"Title\"],\n",
        "        \"abstract\":[\"abstract\",\"snippet\",\"abstract_text\",\"lead_paragraph\",\"Summary\"],\n",
        "        \"news_desk\":[\"news_desk\",\"desk\"],\n",
        "        \"type_of_material\":[\"type_of_material\",\"type\",\"material_type_facet\"],\n",
        "        \"keywords\":[\"keywords\",\"descriptors\",\"subject\",\"keywords_list\"],\n",
        "    }\n",
        "    for canon, opts in cand.items():\n",
        "        for o in opts:\n",
        "            if o in df.columns:\n",
        "                df[canon] = df[o]; break\n",
        "    if \"url\" in df: df[\"url\"] = df[\"url\"].map(_clean_url)\n",
        "    if \"keywords\" in df: df[\"keywords\"] = df[\"keywords\"].map(_parse_keywords)\n",
        "    else: df[\"keywords\"] = [[] for _ in range(len(df))]\n",
        "    for c in ARTICLE_COLS:\n",
        "        if c not in df: df[c] = pd.NA\n",
        "    return df[ARTICLE_COLS]\n",
        "\n",
        "def normalize_comments(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    cand = {\n",
        "        \"comment_id\":[\"commentID\",\"comment_id\",\"id\"],\n",
        "        \"parent_id\":[\"parentID\",\"parent_id\",\"inReplyTo\"],\n",
        "        \"user\":[\"userDisplayName\",\"user\",\"display_name\",\"author\"],\n",
        "        \"comment\":[\"commentBody\",\"comment\",\"text\",\"body\"],\n",
        "        \"recommendations\":[\"recommendations\",\"recommendCount\",\"recommendedCount\"],\n",
        "        \"reply_count\":[\"replyCount\",\"replies\",\"numReplies\"],\n",
        "        \"editors_selection\":[\"editorsSelection\",\"editors_selection\"],\n",
        "        \"date\":[\"createDate\",\"date\",\"timestamp\"],\n",
        "        \"article_url\":[\"articleURL\",\"url\",\"articleUrl\",\"web_url\"],\n",
        "        \"article_id\":[\"articleID\",\"asset_id\",\"assetId\",\"article_id\"],\n",
        "    }\n",
        "    for canon, opts in cand.items():\n",
        "        for o in opts:\n",
        "            if o in df.columns:\n",
        "                df[canon] = df[o]; break\n",
        "    if \"article_url\" in df: df[\"article_url\"] = df[\"article_url\"].map(_clean_url)\n",
        "    for c in COMMENT_COLS:\n",
        "        if c not in df: df[c] = pd.NA\n",
        "    return df[COMMENT_COLS]\n",
        "\n",
        "SECTION_ALLOW = {\"u.s.\",\"us\",\"us news\",\"politics\",\"business\",\"economy\"}\n",
        "KEYWORD_PATTERNS = [\n",
        "    # Politics\n",
        "    r\"\\bpolitic(s|al)?\\b\", r\"\\bcongress\\b\", r\"\\bsenate\\b\", r\"\\bhouse of representatives\\b\",\n",
        "    r\"\\bwhite house\\b\", r\"\\bpresident\\b\", r\"\\bgovern(or|ment)\\b\", r\"\\bgubernatorial\\b\",\n",
        "    r\"\\belection(s)?\\b\", r\"\\bprimary\\b\", r\"\\bmidterm(s)?\\b\", r\"\\bballot\\b\", r\"\\bcampaign\\b\",\n",
        "    r\"\\bsupreme court\\b\", r\"\\bscotus\\b\", r\"\\bjustice department\\b\", r\"\\battorney general\\b\",\n",
        "    r\"\\bimpeachment\\b\", r\"\\blobby(ist|ing)\\b\", r\"\\bpublic policy\\b\",\n",
        "    # Economy\n",
        "    r\"\\beconom(y|ic|ics)\\b\", r\"\\bgdp\\b\", r\"\\binflation\\b\", r\"\\bunemployment\\b\",\n",
        "    r\"\\bjobs? report\\b\", r\"\\bfederal reserve\\b\", r\"\\binterest rate(s)?\\b\",\n",
        "    r\"\\bstimulus\\b\", r\"\\btrade\\b\", r\"\\btariff(s)?\\b\", r\"\\bmanufactur(ing|ers?)\\b\",\n",
        "    r\"\\bconsumer confidence\\b\", r\"\\bretail sales\\b\", r\"\\blabor market\\b\", r\"\\bdeficit\\b\", r\"\\bnational debt\\b\",\n",
        "]\n",
        "KEYWORD_REGEX = re.compile(\"|\".join(KEYWORD_PATTERNS), re.IGNORECASE)\n",
        "\n",
        "def _is_domestic(row):\n",
        "    # NA-safe checks\n",
        "    sec = _to_lower_str(row.get(\"section\"))\n",
        "    if sec in SECTION_ALLOW:\n",
        "        return True\n",
        "    sub = _to_lower_str(row.get(\"subsection\"))\n",
        "    if any(x in sub for x in [\"politic\",\"u.s.\",\"business\",\"economy\"]):\n",
        "        return True\n",
        "\n",
        "    kws = row.get(\"keywords\", [])\n",
        "    if isinstance(kws, list):\n",
        "        for k in kws:\n",
        "            if KEYWORD_REGEX.search(_to_lower_str(k)):\n",
        "                return True\n",
        "    else:\n",
        "        if KEYWORD_REGEX.search(_to_lower_str(kws)):\n",
        "            return True\n",
        "\n",
        "    for col in (\"headline\",\"abstract\"):\n",
        "        if KEYWORD_REGEX.search(_to_lower_str(row.get(col))):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# --------- dataset loaders (articles fully; comments chunked) ----------\n",
        "def load_all_articles(dirpath: str) -> pd.DataFrame:\n",
        "    # articles are relatively small; loading all is fine\n",
        "    paths = sorted(set(\n",
        "        glob.glob(os.path.join(dirpath, \"Articles*.csv\")) +\n",
        "        glob.glob(os.path.join(dirpath, \"*articles*.csv\")) +\n",
        "        glob.glob(os.path.join(dirpath, \"articles*.csv\"))\n",
        "    ))\n",
        "    if not paths:\n",
        "        raise FileNotFoundError(f\"No article CSVs in {dirpath}\")\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in paths], ignore_index=True)\n",
        "    arts = normalize_articles(arts)\n",
        "    # Ensure URL is unique-ish; keep first occurrence if duplicates\n",
        "    arts = arts.sort_values(\"pub_date\").drop_duplicates(subset=[\"article_id\",\"url\"], keep=\"first\")\n",
        "    return arts\n",
        "\n",
        "def comment_paths(dirpath: str) -> List[str]:\n",
        "    paths = sorted(set(\n",
        "        glob.glob(os.path.join(dirpath, \"Comments*.csv\")) +\n",
        "        glob.glob(os.path.join(dirpath, \"*comments*.csv\")) +\n",
        "        glob.glob(os.path.join(dirpath, \"comments*.csv\"))\n",
        "    ))\n",
        "    if not paths:\n",
        "        raise FileNotFoundError(f\"No comment CSVs in {dirpath}\")\n",
        "    return paths\n",
        "\n",
        "def process_comments_chunked(articles: pd.DataFrame, cpaths: List[str], out_joined_csv: Optional[str],\n",
        "                             out_filtered_csv: str, chunksize: int = 200_000):\n",
        "    os.makedirs(os.path.dirname(out_filtered_csv), exist_ok=True)\n",
        "    # Prepare output: remove old files if re-running\n",
        "    for f in [out_joined_csv, out_filtered_csv]:\n",
        "        if f and os.path.exists(f):\n",
        "            os.remove(f)\n",
        "\n",
        "    # Build fast lookup by URL for fill step (article columns we need)\n",
        "    art_keep = [\"url\",\"article_id\",\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\"]\n",
        "    arts_small = articles[art_keep].copy()\n",
        "\n",
        "    for p in cpaths:\n",
        "        print(f\"Processing comments file: {os.path.basename(p)}\")\n",
        "        for i, chunk in enumerate(pd.read_csv(p, low_memory=False, chunksize=chunksize)):\n",
        "            c = normalize_comments(chunk)\n",
        "\n",
        "            # 1) Merge on article_id when present\n",
        "            m1 = c.merge(articles, how=\"left\", on=\"article_id\", suffixes=(\"_c\",\"_a\"))\n",
        "\n",
        "            # 2) For rows still missing article info, fill by URL (index-aligned; NA-safe)\n",
        "            missing = m1[\"headline\"].isna()\n",
        "            if missing.any():\n",
        "                missing_idx = m1.index[missing]\n",
        "                need = m1.loc[missing_idx, [\"article_url\"]].copy()\n",
        "                m2 = need.merge(\n",
        "                    arts_small, how=\"left\", left_on=\"article_url\", right_on=\"url\"\n",
        "                ).drop(columns=[\"url\"])\n",
        "                cols_to_fill = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\n",
        "                                \"news_desk\",\"type_of_material\",\"keywords\",\"article_id\"]\n",
        "                for col in cols_to_fill:\n",
        "                    target = m1.loc[missing_idx, col]\n",
        "                    filler = m2[col]\n",
        "                    to_replace = target.isna()\n",
        "                    if to_replace.any():\n",
        "                        m1.loc[missing_idx[to_replace], col] = filler[to_replace].values\n",
        "\n",
        "            # write joined (optional â€“ big)\n",
        "            if out_joined_csv:\n",
        "                m1.to_csv(out_joined_csv, mode=\"a\", header=not os.path.exists(out_joined_csv), index=False)\n",
        "\n",
        "            # filter and append\n",
        "            filt = m1[m1.apply(_is_domestic, axis=1)]\n",
        "            if len(filt):\n",
        "                filt.to_csv(out_filtered_csv, mode=\"a\", header=not os.path.exists(out_filtered_csv), index=False)\n",
        "\n",
        "            print(f\"  chunk {i+1}: joined={len(m1):,}, kept={len(filt):,}\")\n",
        "\n",
        "# --------- master runner ----------\n",
        "def build_dataset_chunked(path_17_18: str, path_2020: str, out_dir=\"nyt_outputs\", chunksize=200_000,\n",
        "                          write_joined=False):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 2017/2018\n",
        "    artsA = load_all_articles(path_17_18)\n",
        "    cpathsA = comment_paths(path_17_18)\n",
        "    joinedA = os.path.join(out_dir, \"joined_2017_2018_all.csv\") if write_joined else None\n",
        "    filtA   = os.path.join(out_dir, \"domestic_politics_econ_2017_2018.csv\")\n",
        "    process_comments_chunked(artsA, cpathsA, joinedA, filtA, chunksize=chunksize)\n",
        "\n",
        "    # 2020\n",
        "    artsB = load_all_articles(path_2020)\n",
        "    cpathsB = comment_paths(path_2020)\n",
        "    joinedB = os.path.join(out_dir, \"joined_2020_all.csv\") if write_joined else None\n",
        "    filtB   = os.path.join(out_dir, \"domestic_politics_econ_2020.csv\")\n",
        "    process_comments_chunked(artsB, cpathsB, joinedB, filtB, chunksize=chunksize)\n",
        "\n",
        "    # Combine filtered pieces into a single file\n",
        "    combo_out = os.path.join(out_dir, \"domestic_politics_econ_filtered.csv\")\n",
        "    if os.path.exists(combo_out): os.remove(combo_out)\n",
        "    for f in [filtA, filtB]:\n",
        "        if os.path.exists(f):\n",
        "            pd.read_csv(f, low_memory=False).to_csv(\n",
        "                combo_out, mode=\"a\", header=not os.path.exists(combo_out), index=False\n",
        "            )\n",
        "    print(\"Done. Outputs in:\", out_dir)\n"
      ],
      "metadata": {
        "id": "GN6wo4WS42NZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path_17_18 = kagglehub.dataset_download(\"aashita/nyt-comments\")\n",
        "path_2020  = kagglehub.dataset_download(\"benjaminawd/new-york-times-articles-comments-2020\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ezcpGHQ3bVu",
        "outputId": "03b81c70-6830-4ca6-df62-982aa497edd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nyt-comments' dataset.\n",
            "Using Colab cache for faster access to the 'new-york-times-articles-comments-2020' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "def _to_lower_str(val):\n",
        "    # robustly convert any value (including pd.NA/NaN/None) to a safe lowercased string\n",
        "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
        "        return \"\"\n",
        "    if val is pd.NA or (hasattr(pd, \"isna\") and pd.isna(val)):\n",
        "        return \"\"\n",
        "    return str(val).strip().lower()\n",
        "\n",
        "def _is_domestic(row):\n",
        "    # section/subsection checks\n",
        "    sec = _to_lower_str(row.get(\"section\"))\n",
        "    if sec in {\"u.s.\", \"us\", \"us news\", \"politics\", \"business\", \"economy\"}:\n",
        "        return True\n",
        "\n",
        "    sub = _to_lower_str(row.get(\"subsection\"))\n",
        "    if any(x in sub for x in [\"politic\", \"u.s.\", \"business\", \"economy\"]):\n",
        "        return True\n",
        "\n",
        "    # keywords/descriptors (could be list, string, or NA)\n",
        "    kws = row.get(\"keywords\", [])\n",
        "    if isinstance(kws, list):\n",
        "        for k in kws:\n",
        "            txt = _to_lower_str(k)\n",
        "            if KEYWORD_REGEX.search(txt):\n",
        "                return True\n",
        "    else:\n",
        "        txt = _to_lower_str(kws)\n",
        "        if KEYWORD_REGEX.search(txt):\n",
        "            return True\n",
        "\n",
        "    # headline/abstract fallback\n",
        "    for col in (\"headline\", \"abstract\"):\n",
        "        txt = _to_lower_str(row.get(col))\n",
        "        if KEYWORD_REGEX.search(txt):\n",
        "            return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "9rPovWrG3Ph3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_dataset_chunked(\n",
        "    path_17_18,\n",
        "    path_2020,\n",
        "    out_dir=\"/content/nyt_outputs\",\n",
        "    chunksize=50_000,   # lower to 100_000 if memory is tight\n",
        "    write_joined=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj3NIc4U3QfF",
        "outputId": "290e9d7d-bac2-4cc5-caa8-84f6b42559d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing comments file: CommentsApril2017.csv\n",
            "  chunk 1: joined=50,000, kept=26,942\n",
            "  chunk 2: joined=50,000, kept=21,229\n",
            "  chunk 3: joined=50,000, kept=32,515\n",
            "  chunk 4: joined=50,000, kept=31,170\n",
            "  chunk 5: joined=43,832, kept=22,714\n",
            "Processing comments file: CommentsApril2018.csv\n",
            "  chunk 1: joined=50,000, kept=30,713\n",
            "  chunk 2: joined=50,000, kept=38,189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ULTRA-LOW-MEM NYT join+filter (dict maps; no big merges) ---\n",
        "import os, re, glob, math, gc\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "# --------- helpers ----------\n",
        "def _clean_url(u):\n",
        "    if pd.isna(u): return u\n",
        "    return str(u).strip().split('?')[0].rstrip('/').lower()\n",
        "\n",
        "def _to_lower_str(val):\n",
        "    if val is None or val is pd.NA: return \"\"\n",
        "    try:\n",
        "        if isinstance(val, float) and math.isnan(val): return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    if hasattr(pd, \"isna\") and pd.isna(val): return \"\"\n",
        "    return str(val).strip().lower()\n",
        "\n",
        "SECTION_ALLOW = {\"u.s.\",\"us\",\"us news\",\"politics\",\"business\",\"economy\"}\n",
        "KEYWORD_PATTERNS = [\n",
        "    # Politics\n",
        "    r\"\\bpolitic(s|al)?\\b\", r\"\\bcongress\\b\", r\"\\bsenate\\b\", r\"\\bhouse of representatives\\b\",\n",
        "    r\"\\bwhite house\\b\", r\"\\bpresident\\b\", r\"\\bgovern(or|ment)\\b\", r\"\\bgubernatorial\\b\",\n",
        "    r\"\\belection(s)?\\b\", r\"\\bprimary\\b\", r\"\\bmidterm(s)?\\b\", r\"\\bballot\\b\", r\"\\bcampaign\\b\",\n",
        "    r\"\\bsupreme court\\b\", r\"\\bscotus\\b\", r\"\\bjustice department\\b\", r\"\\battorney general\\b\",\n",
        "    r\"\\bimpeachment\\b\", r\"\\blobby(ist|ing)\\b\", r\"\\bpublic policy\\b\",\n",
        "    # Economy\n",
        "    r\"\\beconom(y|ic|ics)\\b\", r\"\\bgdp\\b\", r\"\\binflation\\b\", r\"\\bunemployment\\b\",\n",
        "    r\"\\bjobs? report\\b\", r\"\\bfederal reserve\\b\", r\"\\binterest rate(s)?\\b\",\n",
        "    r\"\\bstimulus\\b\", r\"\\btrade\\b\", r\"\\btariff(s)?\\b\", r\"\\bmanufactur(ing|ers?)\\b\",\n",
        "    r\"\\bconsumer confidence\\b\", r\"\\bretail sales\\b\", r\"\\blabor market\\b\", r\"\\bdeficit\\b\", r\"\\bnational debt\\b\",\n",
        "]\n",
        "KEYWORD_REGEX = re.compile(\"|\".join(KEYWORD_PATTERNS), re.IGNORECASE)\n",
        "\n",
        "# Columns we want to preserve in output\n",
        "OUT_COLS = [\n",
        "    # comment-side\n",
        "    \"comment_id\",\"parent_id\",\"user\",\"comment\",\"recommendations\",\"reply_count\",\n",
        "    \"editors_selection\",\"date\",\n",
        "    # article-side\n",
        "    \"article_id\",\"article_url\",\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\n",
        "    \"abstract\",\"news_desk\",\"type_of_material\",\"keywords\",\n",
        "]\n",
        "\n",
        "# Flexible name maps for normalization (minimal set)\n",
        "ART_MAP = {\n",
        "    \"article_id\": [\"articleID\",\"asset_id\",\"assetId\",\"id\",\"article_id\"],\n",
        "    \"url\": [\"articleURL\",\"url\",\"web_url\",\"link\"],\n",
        "    \"pub_date\": [\"pub_date\",\"pubDate\",\"date\",\"published_date\",\"PublicationDate\"],\n",
        "    \"section\": [\"section\",\"section_name\",\"newsSection\",\"news_desk\"],\n",
        "    \"subsection\": [\"subsection\",\"subsection_name\",\"sub_section\"],\n",
        "    \"headline\": [\"headline\",\"title\",\"main_headline\",\"headline.main\",\"Title\"],\n",
        "    \"abstract\": [\"abstract\",\"snippet\",\"abstract_text\",\"lead_paragraph\",\"Summary\"],\n",
        "    \"news_desk\": [\"news_desk\",\"desk\"],\n",
        "    \"type_of_material\": [\"type_of_material\",\"type\",\"material_type_facet\"],\n",
        "    \"keywords\": [\"keywords\",\"descriptors\",\"subject\",\"keywords_list\"],\n",
        "}\n",
        "\n",
        "COM_MAP = {\n",
        "    \"comment_id\": [\"commentID\",\"comment_id\",\"id\"],\n",
        "    \"parent_id\": [\"parentID\",\"parent_id\",\"inReplyTo\"],\n",
        "    \"user\": [\"userDisplayName\",\"user\",\"display_name\",\"author\"],\n",
        "    \"comment\": [\"commentBody\",\"comment\",\"text\",\"body\"],\n",
        "    \"recommendations\": [\"recommendations\",\"recommendCount\",\"recommendedCount\"],\n",
        "    \"reply_count\": [\"replyCount\",\"replies\",\"numReplies\"],\n",
        "    \"editors_selection\": [\"editorsSelection\",\"editors_selection\"],\n",
        "    \"date\": [\"createDate\",\"date\",\"timestamp\"],\n",
        "    \"article_url\": [\"articleURL\",\"url\",\"articleUrl\",\"web_url\"],\n",
        "    \"article_id\": [\"articleID\",\"asset_id\",\"assetId\",\"article_id\"],\n",
        "}\n",
        "\n",
        "def _first_present(df, candidates, default=pd.NA):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    return pd.Series([default]*len(df))\n",
        "\n",
        "def _normalize_articles_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"article_id\": _first_present(df, ART_MAP[\"article_id\"]),\n",
        "        \"url\": _first_present(df, ART_MAP[\"url\"]).map(_clean_url),\n",
        "        \"pub_date\": _first_present(df, ART_MAP[\"pub_date\"]),\n",
        "        \"section\": _first_present(df, ART_MAP[\"section\"]),\n",
        "        \"subsection\": _first_present(df, ART_MAP[\"subsection\"]),\n",
        "        \"headline\": _first_present(df, ART_MAP[\"headline\"]),\n",
        "        \"abstract\": _first_present(df, ART_MAP[\"abstract\"]),\n",
        "        \"news_desk\": _first_present(df, ART_MAP[\"news_desk\"]),\n",
        "        \"type_of_material\": _first_present(df, ART_MAP[\"type_of_material\"]),\n",
        "        \"keywords\": _first_present(df, ART_MAP[\"keywords\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def _normalize_comments_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"comment_id\": _first_present(df, COM_MAP[\"comment_id\"]),\n",
        "        \"parent_id\": _first_present(df, COM_MAP[\"parent_id\"]),\n",
        "        \"user\": _first_present(df, COM_MAP[\"user\"]),\n",
        "        \"comment\": _first_present(df, COM_MAP[\"comment\"]),\n",
        "        \"recommendations\": _first_present(df, COM_MAP[\"recommendations\"]),\n",
        "        \"reply_count\": _first_present(df, COM_MAP[\"reply_count\"]),\n",
        "        \"editors_selection\": _first_present(df, COM_MAP[\"editors_selection\"]),\n",
        "        \"date\": _first_present(df, COM_MAP[\"date\"]),\n",
        "        \"article_url\": _first_present(df, COM_MAP[\"article_url\"]).map(_clean_url),\n",
        "        \"article_id\": _first_present(df, COM_MAP[\"article_id\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def _is_domestic_row(section, subsection, headline, abstract, keywords):\n",
        "    sec = _to_lower_str(section)\n",
        "    if sec in SECTION_ALLOW:\n",
        "        return True\n",
        "    sub = _to_lower_str(subsection)\n",
        "    if any(x in sub for x in [\"politic\",\"u.s.\",\"business\",\"economy\"]):\n",
        "        return True\n",
        "    # keywords may be list-like or string; treat as string\n",
        "    if isinstance(keywords, list):\n",
        "        kwtxt = \" \".join(_to_lower_str(k) for k in keywords if k is not None)\n",
        "    else:\n",
        "        kwtxt = _to_lower_str(keywords)\n",
        "    if KEYWORD_REGEX.search(kwtxt or \"\"):\n",
        "        return True\n",
        "    if KEYWORD_REGEX.search(_to_lower_str(headline) or \"\"):\n",
        "        return True\n",
        "    if KEYWORD_REGEX.search(_to_lower_str(abstract) or \"\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# --------- dataset loaders ----------\n",
        "def _find(paths_patterns: List[str]) -> List[str]:\n",
        "    s = set()\n",
        "    for pat in paths_patterns:\n",
        "        s.update(glob.glob(pat))\n",
        "    return sorted(s)\n",
        "\n",
        "def load_articles_build_maps(dirpath: str):\n",
        "    a_paths = _find([\n",
        "        os.path.join(dirpath, \"Articles*.csv\"),\n",
        "        os.path.join(dirpath, \"*articles*.csv\"),\n",
        "        os.path.join(dirpath, \"articles*.csv\"),\n",
        "    ])\n",
        "    if not a_paths:\n",
        "        raise FileNotFoundError(f\"No article CSVs in {dirpath}\")\n",
        "\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in a_paths], ignore_index=True)\n",
        "    arts = _normalize_articles_small(arts)\n",
        "\n",
        "    # Drop dups, keep earliest by pub_date\n",
        "    arts = arts.sort_values(\"pub_date\").drop_duplicates(subset=[\"article_id\",\"url\"], keep=\"first\")\n",
        "\n",
        "    # Build compact tuples for map\n",
        "    cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "    tup = arts[cols].itertuples(index=False, name=None)\n",
        "\n",
        "    by_id: Dict[str, Tuple] = {}\n",
        "    if \"article_id\" in arts.columns:\n",
        "        for aid, t in zip(arts[\"article_id\"], tup):\n",
        "            if pd.isna(aid): continue\n",
        "            by_id[str(aid)] = t\n",
        "    # rebuild tup iterator for url map\n",
        "    tup2 = arts[cols].itertuples(index=False, name=None)\n",
        "    by_url: Dict[str, Tuple] = {}\n",
        "    for u, t in zip(arts[\"url\"], tup2):\n",
        "        if pd.isna(u): continue\n",
        "        by_url[str(u)] = t\n",
        "\n",
        "    return by_id, by_url\n",
        "\n",
        "def comment_files(dirpath: str) -> List[str]:\n",
        "    c_paths = _find([\n",
        "        os.path.join(dirpath, \"Comments*.csv\"),\n",
        "        os.path.join(dirpath, \"*comments*.csv\"),\n",
        "        os.path.join(dirpath, \"comments*.csv\"),\n",
        "    ])\n",
        "    if not c_paths:\n",
        "        raise FileNotFoundError(f\"No comment CSVs in {dirpath}\")\n",
        "    return c_paths\n",
        "\n",
        "# --------- streaming join/filter ----------\n",
        "def process_comment_stream(by_id, by_url, cpaths: List[str], out_filtered_csv: str,\n",
        "                           chunksize: int = 25_000):\n",
        "    os.makedirs(os.path.dirname(out_filtered_csv), exist_ok=True)\n",
        "    if os.path.exists(out_filtered_csv):\n",
        "        os.remove(out_filtered_csv)\n",
        "\n",
        "    # Only read the minimal set of columns we care about\n",
        "    allowed = set(sum(COM_MAP.values(), []))  # flatten list of lists\n",
        "    usecols = lambda c: c in allowed  # callable avoids errors if some don't exist\n",
        "\n",
        "    for p in cpaths:\n",
        "        print(f\"Processing: {os.path.basename(p)}\")\n",
        "        for i, raw in enumerate(pd.read_csv(p, low_memory=False, chunksize=chunksize, usecols=usecols)):\n",
        "            c = _normalize_comments_small(raw)\n",
        "\n",
        "            # Prepare article fields from ID map\n",
        "            aid = c[\"article_id\"].astype(str)\n",
        "            got = aid.map(by_id)  # tuple or NaN\n",
        "\n",
        "            # Create empty columns\n",
        "            for col in [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]:\n",
        "                c[col] = pd.NA\n",
        "\n",
        "            # Fill from ID hits\n",
        "            mask_id = got.notna()\n",
        "            if mask_id.any():\n",
        "                vals = got[mask_id].tolist()\n",
        "                cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "                filled = pd.DataFrame(vals, index=c.index[mask_id], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[mask_id, col] = filled[col].values\n",
        "\n",
        "            # For remaining rows, try URL lookup\n",
        "            mask_need = c[\"headline\"].isna()\n",
        "            if mask_need.any():\n",
        "                url_clean = c.loc[mask_need, \"article_url\"].map(lambda x: _clean_url(x))\n",
        "                got2 = url_clean.map(by_url)\n",
        "                mask_url = got2.notna()\n",
        "                if mask_url.any():\n",
        "                    vals2 = got2[mask_url].tolist()\n",
        "                    filled2 = pd.DataFrame(vals2, index=c.index[mask_need][mask_url], columns=cols)\n",
        "                    for col in cols:\n",
        "                        c.loc[c.index[mask_need][mask_url], col] = filled2[col].values\n",
        "\n",
        "            # Filter rows for domestic politics/economy\n",
        "            keep_mask = c.apply(\n",
        "                lambda r: _is_domestic_row(r[\"section\"], r[\"subsection\"], r[\"headline\"], r[\"abstract\"], r[\"keywords\"]),\n",
        "                axis=1\n",
        "            )\n",
        "            out = c.loc[keep_mask, OUT_COLS].copy()\n",
        "\n",
        "            if len(out):\n",
        "                out.to_csv(out_filtered_csv, mode=\"a\", header=not os.path.exists(out_filtered_csv), index=False)\n",
        "\n",
        "            print(f\"  chunk {i+1}: total={len(c):,}, kept={len(out):,}\")\n",
        "            # free memory\n",
        "            del raw, c, out, got\n",
        "            if 'got2' in locals(): del got2\n",
        "            gc.collect()\n",
        "\n",
        "def build_dataset_ultra(path_2017_2018: str, path_2020: str,\n",
        "                        out_dir=\"/content/nyt_outputs\", chunksize=25_000):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 2017/2018\n",
        "    by_id_A, by_url_A = load_articles_build_maps(path_2017_2018)\n",
        "    filtA = os.path.join(out_dir, \"domestic_politics_econ_2017_2018.csv\")\n",
        "    process_comment_stream(by_id_A, by_url_A, comment_files(path_2017_2018), filtA, chunksize=chunksize)\n",
        "\n",
        "    # 2020\n",
        "    by_id_B, by_url_B = load_articles_build_maps(path_2020)\n",
        "    filtB = os.path.join(out_dir, \"domestic_politics_econ_2020.csv\")\n",
        "    process_comment_stream(by_id_B, by_url_B, comment_files(path_2020), filtB, chunksize=chunksize)\n",
        "\n",
        "    # combine\n",
        "    combo = os.path.join(out_dir, \"domestic_politics_econ_filtered.csv\")\n",
        "    if os.path.exists(combo): os.remove(combo)\n",
        "    for f in [filtA, filtB]:\n",
        "        if os.path.exists(f):\n",
        "            pd.read_csv(f, low_memory=False).to_csv(combo, mode=\"a\", header=not os.path.exists(combo), index=False)\n",
        "\n",
        "    print(\"Done. Outputs in:\", out_dir)\n"
      ],
      "metadata": {
        "id": "JPybkWzk59Xy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path_17_18 = kagglehub.dataset_download(\"aashita/nyt-comments\")\n",
        "path_2020  = kagglehub.dataset_download(\"benjaminawd/new-york-times-articles-comments-2020\")\n",
        "\n",
        "build_dataset_ultra(\n",
        "    path_2017_2018=path_17_18,\n",
        "    path_2020=path_2020,\n",
        "    out_dir=\"/content/nyt_outputs\",\n",
        "    chunksize=25_000  # try 10_000 if T4 is still tight\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qDYrWcP46A17",
        "outputId": "52305054-052e-4a28-f018-1bc588ed543b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nyt-comments' dataset.\n",
            "Using Colab cache for faster access to the 'new-york-times-articles-comments-2020' dataset.\n",
            "Processing: CommentsApril2017.csv\n",
            "  chunk 1: total=25,000, kept=14,057\n",
            "  chunk 2: total=50,000, kept=12,885\n",
            "  chunk 3: total=50,000, kept=12,288\n",
            "  chunk 4: total=50,000, kept=8,941\n",
            "  chunk 5: total=50,000, kept=15,331\n",
            "  chunk 6: total=50,000, kept=17,184\n",
            "  chunk 7: total=50,000, kept=18,535\n",
            "  chunk 8: total=50,000, kept=12,635\n",
            "  chunk 9: total=50,000, kept=13,714\n",
            "  chunk 10: total=37,664, kept=9,000\n",
            "Processing: CommentsApril2018.csv\n",
            "  chunk 1: total=25,000, kept=16,110\n",
            "  chunk 2: total=50,000, kept=14,603\n",
            "  chunk 3: total=50,000, kept=19,113\n",
            "  chunk 4: total=50,000, kept=19,076\n",
            "  chunk 5: total=50,000, kept=12,122\n",
            "  chunk 6: total=50,000, kept=15,023\n",
            "  chunk 7: total=50,000, kept=11,398\n",
            "  chunk 8: total=50,000, kept=13,867\n",
            "  chunk 9: total=50,000, kept=11,870\n",
            "  chunk 10: total=50,000, kept=11,313\n",
            "  chunk 11: total=29,848, kept=8,634\n",
            "Processing: CommentsFeb2017.csv\n",
            "  chunk 1: total=25,000, kept=16,540\n",
            "  chunk 2: total=50,000, kept=18,547\n",
            "  chunk 3: total=50,000, kept=17,918\n",
            "  chunk 4: total=50,000, kept=21,940\n",
            "  chunk 5: total=50,000, kept=18,675\n",
            "  chunk 6: total=50,000, kept=18,857\n",
            "  chunk 7: total=50,000, kept=17,019\n",
            "  chunk 8: total=50,000, kept=16,877\n",
            "  chunk 9: total=50,000, kept=18,206\n",
            "  chunk 10: total=16,814, kept=5,509\n",
            "Processing: CommentsFeb2018.csv\n",
            "  chunk 1: total=25,000, kept=17,929\n",
            "  chunk 2: total=50,000, kept=11,854\n",
            "  chunk 3: total=50,000, kept=16,529\n",
            "  chunk 4: total=50,000, kept=16,146\n",
            "  chunk 5: total=50,000, kept=14,817\n",
            "  chunk 6: total=50,000, kept=13,978\n",
            "  chunk 7: total=50,000, kept=16,311\n",
            "  chunk 8: total=50,000, kept=14,793\n",
            "  chunk 9: total=30,564, kept=7,618\n",
            "Processing: CommentsJan2017.csv\n",
            "  chunk 1: total=25,000, kept=16,740\n",
            "  chunk 2: total=50,000, kept=19,095\n",
            "  chunk 3: total=50,000, kept=14,954\n",
            "  chunk 4: total=50,000, kept=14,074\n",
            "  chunk 5: total=50,000, kept=16,515\n",
            "  chunk 6: total=50,000, kept=18,676\n",
            "  chunk 7: total=50,000, kept=18,060\n",
            "  chunk 8: total=50,000, kept=16,090\n",
            "  chunk 9: total=50,000, kept=16,294\n",
            "  chunk 10: total=12,898, kept=4,839\n",
            "Processing: CommentsJan2018.csv\n",
            "  chunk 1: total=25,000, kept=15,411\n",
            "  chunk 2: total=50,000, kept=14,650\n",
            "  chunk 3: total=50,000, kept=17,262\n",
            "  chunk 4: total=50,000, kept=14,819\n",
            "  chunk 5: total=50,000, kept=14,112\n",
            "  chunk 6: total=50,000, kept=15,497\n",
            "  chunk 7: total=50,000, kept=15,155\n",
            "  chunk 8: total=50,000, kept=1,858\n",
            "  chunk 9: total=6,398, kept=0\n",
            "Processing: CommentsMarch2017.csv\n",
            "  chunk 1: total=25,000, kept=17,745\n",
            "  chunk 2: total=50,000, kept=18,800\n",
            "  chunk 3: total=50,000, kept=16,731\n",
            "  chunk 4: total=50,000, kept=18,122\n",
            "  chunk 5: total=50,000, kept=17,848\n",
            "  chunk 6: total=50,000, kept=18,219\n",
            "  chunk 7: total=50,000, kept=16,308\n",
            "  chunk 8: total=50,000, kept=17,164\n",
            "  chunk 9: total=50,000, kept=15,133\n",
            "  chunk 10: total=50,000, kept=17,066\n",
            "  chunk 11: total=21,934, kept=7,311\n",
            "Processing: CommentsMarch2018.csv\n",
            "  chunk 1: total=25,000, kept=16,557\n",
            "  chunk 2: total=50,000, kept=15,478\n",
            "  chunk 3: total=50,000, kept=15,506\n",
            "  chunk 4: total=50,000, kept=15,962\n",
            "  chunk 5: total=50,000, kept=16,403\n",
            "  chunk 6: total=50,000, kept=17,432\n",
            "  chunk 7: total=50,000, kept=13,721\n",
            "  chunk 8: total=50,000, kept=14,450\n",
            "  chunk 9: total=50,000, kept=14,383\n",
            "  chunk 10: total=43,830, kept=11,963\n",
            "Processing: CommentsMay2017.csv\n",
            "  chunk 1: total=25,000, kept=17,966\n",
            "  chunk 2: total=50,000, kept=19,187\n",
            "  chunk 3: total=50,000, kept=14,679\n",
            "  chunk 4: total=50,000, kept=19,702\n",
            "  chunk 5: total=50,000, kept=18,826\n",
            "  chunk 6: total=50,000, kept=17,865\n",
            "  chunk 7: total=50,000, kept=18,456\n",
            "  chunk 8: total=50,000, kept=16,611\n",
            "  chunk 9: total=50,000, kept=13,554\n",
            "  chunk 10: total=50,000, kept=19,042\n",
            "  chunk 11: total=50,000, kept=19,900\n",
            "  chunk 12: total=2,778, kept=1,338\n",
            "Processing: nyt-comments-2020.csv\n",
            "  chunk 1: total=25,000, kept=0\n",
            "  chunk 2: total=50,000, kept=0\n",
            "  chunk 3: total=50,000, kept=0\n",
            "  chunk 4: total=50,000, kept=0\n",
            "  chunk 5: total=50,000, kept=0\n",
            "  chunk 6: total=50,000, kept=0\n",
            "  chunk 7: total=50,000, kept=0\n",
            "  chunk 8: total=50,000, kept=0\n",
            "  chunk 9: total=50,000, kept=0\n",
            "  chunk 10: total=50,000, kept=0\n",
            "  chunk 11: total=50,000, kept=0\n",
            "  chunk 12: total=50,000, kept=0\n",
            "  chunk 13: total=50,000, kept=0\n",
            "  chunk 14: total=50,000, kept=0\n",
            "  chunk 15: total=50,000, kept=0\n",
            "  chunk 16: total=50,000, kept=0\n",
            "  chunk 17: total=50,000, kept=0\n",
            "  chunk 18: total=50,000, kept=0\n",
            "  chunk 19: total=50,000, kept=0\n",
            "  chunk 20: total=50,000, kept=0\n",
            "  chunk 21: total=50,000, kept=0\n",
            "  chunk 22: total=50,000, kept=0\n",
            "  chunk 23: total=50,000, kept=0\n",
            "  chunk 24: total=50,000, kept=0\n",
            "  chunk 25: total=50,000, kept=0\n",
            "  chunk 26: total=50,000, kept=0\n",
            "  chunk 27: total=50,000, kept=0\n",
            "  chunk 28: total=50,000, kept=0\n",
            "  chunk 29: total=50,000, kept=0\n",
            "  chunk 30: total=50,000, kept=0\n",
            "  chunk 31: total=50,000, kept=0\n",
            "  chunk 32: total=50,000, kept=0\n",
            "  chunk 33: total=50,000, kept=0\n",
            "  chunk 34: total=50,000, kept=0\n",
            "  chunk 35: total=50,000, kept=0\n",
            "  chunk 36: total=50,000, kept=0\n",
            "  chunk 37: total=50,000, kept=0\n",
            "  chunk 38: total=50,000, kept=0\n",
            "  chunk 39: total=50,000, kept=0\n",
            "  chunk 40: total=50,000, kept=0\n",
            "  chunk 41: total=50,000, kept=0\n",
            "  chunk 42: total=50,000, kept=0\n",
            "  chunk 43: total=50,000, kept=0\n",
            "  chunk 44: total=50,000, kept=0\n",
            "  chunk 45: total=50,000, kept=0\n",
            "  chunk 46: total=50,000, kept=0\n",
            "  chunk 47: total=50,000, kept=0\n",
            "  chunk 48: total=50,000, kept=0\n",
            "  chunk 49: total=50,000, kept=0\n",
            "  chunk 50: total=50,000, kept=0\n",
            "  chunk 51: total=50,000, kept=0\n",
            "  chunk 52: total=50,000, kept=0\n",
            "  chunk 53: total=50,000, kept=0\n",
            "  chunk 54: total=50,000, kept=0\n",
            "  chunk 55: total=50,000, kept=0\n",
            "  chunk 56: total=50,000, kept=0\n",
            "  chunk 57: total=50,000, kept=0\n",
            "  chunk 58: total=50,000, kept=0\n",
            "  chunk 59: total=50,000, kept=0\n",
            "  chunk 60: total=50,000, kept=0\n",
            "  chunk 61: total=50,000, kept=0\n",
            "  chunk 62: total=50,000, kept=0\n",
            "  chunk 63: total=50,000, kept=0\n",
            "  chunk 64: total=50,000, kept=0\n",
            "  chunk 65: total=50,000, kept=0\n",
            "  chunk 66: total=50,000, kept=0\n",
            "  chunk 67: total=50,000, kept=0\n",
            "  chunk 68: total=50,000, kept=0\n",
            "  chunk 69: total=50,000, kept=0\n",
            "  chunk 70: total=50,000, kept=0\n",
            "  chunk 71: total=50,000, kept=0\n",
            "  chunk 72: total=50,000, kept=0\n",
            "  chunk 73: total=50,000, kept=0\n",
            "  chunk 74: total=50,000, kept=0\n",
            "  chunk 75: total=50,000, kept=0\n",
            "  chunk 76: total=50,000, kept=0\n",
            "  chunk 77: total=50,000, kept=0\n",
            "  chunk 78: total=50,000, kept=0\n",
            "  chunk 79: total=50,000, kept=0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2445716666.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_2020\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benjaminawd/new-york-times-articles-comments-2020\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m build_dataset_ultra(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpath_2017_2018\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_17_18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath_2020\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1743271143.py\u001b[0m in \u001b[0;36mbuild_dataset_ultra\u001b[0;34m(path_2017_2018, path_2020, out_dir, chunksize)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mby_id_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_url_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_articles_build_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mfiltB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"domestic_politics_econ_2020.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mprocess_comment_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby_id_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_url_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# combine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1743271143.py\u001b[0m in \u001b[0;36mprocess_comment_stream\u001b[0;34m(by_id, by_url, cpaths, out_filtered_csv, chunksize)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# Filter rows for domestic politics/economy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             keep_mask = c.apply(\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_is_domestic_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subsection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1743271143.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# Filter rows for domestic politics/economy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             keep_mask = c.apply(\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_is_domestic_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subsection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# Convert generator to list before going through hashable part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3775\u001b[0m     \u001b[0;31m# Indexing Methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3777\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3778\u001b[0m         \"\"\"\n\u001b[1;32m   3779\u001b[0m         \u001b[0mGet\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mboolean\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ULTRA-LOW-MEM NYT join+filter (robust ID/URL maps + URL-aware filter) ---\n",
        "import os, re, glob, math, gc\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "# --------- helpers ----------\n",
        "def _clean_url(u):\n",
        "    if pd.isna(u): return u\n",
        "    return str(u).strip().split('?')[0].rstrip('/').lower()\n",
        "\n",
        "def _to_lower_str(val):\n",
        "    if val is None or val is pd.NA: return \"\"\n",
        "    try:\n",
        "        if isinstance(val, float) and math.isnan(val): return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    if hasattr(pd, \"isna\") and pd.isna(val): return \"\"\n",
        "    return str(val).strip().lower()\n",
        "\n",
        "SECTION_ALLOW = {\"u.s.\",\"us\",\"us news\",\"politics\",\"business\",\"economy\"}\n",
        "KEYWORD_PATTERNS = [\n",
        "    # Politics\n",
        "    r\"\\bpolitic(s|al)?\\b\", r\"\\bcongress\\b\", r\"\\bsenate\\b\", r\"\\bhouse of representatives\\b\",\n",
        "    r\"\\bwhite house\\b\", r\"\\bpresident\\b\", r\"\\bgovern(or|ment)\\b\", r\"\\bgubernatorial\\b\",\n",
        "    r\"\\belection(s)?\\b\", r\"\\bprimary\\b\", r\"\\bmidterm(s)?\\b\", r\"\\bballot\\b\", r\"\\bcampaign\\b\",\n",
        "    r\"\\bsupreme court\\b\", r\"\\bscotus\\b\", r\"\\bjustice department\\b\", r\"\\battorney general\\b\",\n",
        "    r\"\\bimpeachment\\b\", r\"\\blobby(ist|ing)\\b\", r\"\\bpublic policy\\b\",\n",
        "    # Economy\n",
        "    r\"\\beconom(y|ic|ics)\\b\", r\"\\bgdp\\b\", r\"\\binflation\\b\", r\"\\bunemployment\\b\",\n",
        "    r\"\\bjobs? report\\b\", r\"\\bfederal reserve\\b\", r\"\\binterest rate(s)?\\b\",\n",
        "    r\"\\bstimulus\\b\", r\"\\btrade\\b\", r\"\\btariff(s)?\\b\", r\"\\bmanufactur(ing|ers?)\\b\",\n",
        "    r\"\\bconsumer confidence\\b\", r\"\\bretail sales\\b\", r\"\\blabor market\\b\", r\"\\bdeficit\\b\", r\"\\bnational debt\\b\",\n",
        "]\n",
        "KEYWORD_REGEX = re.compile(\"|\".join(KEYWORD_PATTERNS), re.IGNORECASE)\n",
        "\n",
        "# Columns we want to preserve in output\n",
        "OUT_COLS = [\n",
        "    # comment-side\n",
        "    \"comment_id\",\"parent_id\",\"user\",\"comment\",\"recommendations\",\"reply_count\",\n",
        "    \"editors_selection\",\"date\",\n",
        "    # article-side\n",
        "    \"article_id\",\"article_url\",\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\n",
        "    \"abstract\",\"news_desk\",\"type_of_material\",\"keywords\",\n",
        "]\n",
        "\n",
        "# Flexible name maps for normalization (minimal set)\n",
        "ART_MAP = {\n",
        "    \"article_id\": [\"articleID\",\"asset_id\",\"assetId\",\"id\",\"article_id\"],\n",
        "    \"url\": [\"articleURL\",\"url\",\"web_url\",\"link\"],\n",
        "    \"pub_date\": [\"pub_date\",\"pubDate\",\"date\",\"published_date\",\"PublicationDate\"],\n",
        "    \"section\": [\"section\",\"section_name\",\"newsSection\",\"news_desk\"],\n",
        "    \"subsection\": [\"subsection\",\"subsection_name\",\"sub_section\"],\n",
        "    \"headline\": [\"headline\",\"title\",\"main_headline\",\"headline.main\",\"Title\"],\n",
        "    \"abstract\": [\"abstract\",\"snippet\",\"abstract_text\",\"lead_paragraph\",\"Summary\"],\n",
        "    \"news_desk\": [\"news_desk\",\"desk\"],\n",
        "    \"type_of_material\": [\"type_of_material\",\"type\",\"material_type_facet\"],\n",
        "    \"keywords\": [\"keywords\",\"descriptors\",\"subject\",\"keywords_list\"],\n",
        "}\n",
        "\n",
        "COM_MAP = {\n",
        "    \"comment_id\": [\"commentID\",\"comment_id\",\"id\"],\n",
        "    \"parent_id\": [\"parentID\",\"parent_id\",\"inReplyTo\"],\n",
        "    \"user\": [\"userDisplayName\",\"user\",\"display_name\",\"author\"],\n",
        "    \"comment\": [\"commentBody\",\"comment\",\"text\",\"body\"],\n",
        "    \"recommendations\": [\"recommendations\",\"recommendCount\",\"recommendedCount\"],\n",
        "    \"reply_count\": [\"replyCount\",\"replies\",\"numReplies\"],\n",
        "    \"editors_selection\": [\"editorsSelection\",\"editors_selection\"],\n",
        "    \"date\": [\"createDate\",\"date\",\"timestamp\"],\n",
        "    \"article_url\": [\"articleURL\",\"url\",\"articleUrl\",\"web_url\"],\n",
        "    \"article_id\": [\"articleID\",\"asset_id\",\"assetId\",\"article_id\"],\n",
        "}\n",
        "\n",
        "def _first_present(df, candidates, default=pd.NA):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    return pd.Series([default]*len(df))\n",
        "\n",
        "def _normalize_articles_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"article_id\": _first_present(df, ART_MAP[\"article_id\"]),\n",
        "        \"url\": _first_present(df, ART_MAP[\"url\"]).map(_clean_url),\n",
        "        \"pub_date\": _first_present(df, ART_MAP[\"pub_date\"]),\n",
        "        \"section\": _first_present(df, ART_MAP[\"section\"]),\n",
        "        \"subsection\": _first_present(df, ART_MAP[\"subsection\"]),\n",
        "        \"headline\": _first_present(df, ART_MAP[\"headline\"]),\n",
        "        \"abstract\": _first_present(df, ART_MAP[\"abstract\"]),\n",
        "        \"news_desk\": _first_present(df, ART_MAP[\"news_desk\"]),\n",
        "        \"type_of_material\": _first_present(df, ART_MAP[\"type_of_material\"]),\n",
        "        \"keywords\": _first_present(df, ART_MAP[\"keywords\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def _normalize_comments_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"comment_id\": _first_present(df, COM_MAP[\"comment_id\"]),\n",
        "        \"parent_id\": _first_present(df, COM_MAP[\"parent_id\"]),\n",
        "        \"user\": _first_present(df, COM_MAP[\"user\"]),\n",
        "        \"comment\": _first_present(df, COM_MAP[\"comment\"]),\n",
        "        \"recommendations\": _first_present(df, COM_MAP[\"recommendations\"]),\n",
        "        \"reply_count\": _first_present(df, COM_MAP[\"reply_count\"]),\n",
        "        \"editors_selection\": _first_present(df, COM_MAP[\"editors_selection\"]),\n",
        "        \"date\": _first_present(df, COM_MAP[\"date\"]),\n",
        "        \"article_url\": _first_present(df, COM_MAP[\"article_url\"]).map(_clean_url),\n",
        "        \"article_id\": _first_present(df, COM_MAP[\"article_id\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def _is_domestic_row(section, subsection, headline, abstract, keywords, url_hint=None, url_hint2=None):\n",
        "    def url_says_domestic(u):\n",
        "        u = _to_lower_str(u)\n",
        "        if not u:\n",
        "            return False\n",
        "        # section paths\n",
        "        if any(seg in u for seg in [\"/politics/\", \"/us/\", \"/business/\", \"/economy/\"]):\n",
        "            return True\n",
        "        # slug keywords\n",
        "        if any(tok in u for tok in [\n",
        "            \"supreme-court\", \"congress\", \"senate\", \"house-of-representatives\", \"white-house\",\n",
        "            \"president\", \"governor\", \"election\", \"primary\", \"midterm\", \"ballot\", \"campaign\",\n",
        "            \"federal-reserve\", \"inflation\", \"unemployment\", \"jobs-report\", \"interest-rate\",\n",
        "            \"stimulus\", \"trade\", \"tariff\", \"manufactur\", \"consumer-confidence\", \"retail-sales\",\n",
        "            \"labor-market\", \"deficit\", \"national-debt\"\n",
        "        ]):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # 1) Section/subsection checks\n",
        "    sec = _to_lower_str(section)\n",
        "    if sec in SECTION_ALLOW:\n",
        "        return True\n",
        "    sub = _to_lower_str(subsection)\n",
        "    if any(x in sub for x in [\"politic\",\"u.s.\",\"business\",\"economy\"]):\n",
        "        return True\n",
        "\n",
        "    # 2) Keywords/headline/abstract regex\n",
        "    if isinstance(keywords, list):\n",
        "        kwtxt = \" \".join(_to_lower_str(k) for k in keywords if k is not None)\n",
        "    else:\n",
        "        kwtxt = _to_lower_str(keywords)\n",
        "    if KEYWORD_REGEX.search(kwtxt or \"\"):\n",
        "        return True\n",
        "    if KEYWORD_REGEX.search(_to_lower_str(headline) or \"\"):\n",
        "        return True\n",
        "    if KEYWORD_REGEX.search(_to_lower_str(abstract) or \"\"):\n",
        "        return True\n",
        "\n",
        "    # 3) URL-based fallback\n",
        "    if url_says_domestic(url_hint) or url_says_domestic(url_hint2):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# --------- dataset loaders ----------\n",
        "def _find(paths_patterns: List[str]) -> List[str]:\n",
        "    s = set()\n",
        "    for pat in paths_patterns:\n",
        "        s.update(glob.glob(pat))\n",
        "    return sorted(s)\n",
        "\n",
        "def load_articles_build_maps(dirpath: str):\n",
        "    a_paths = _find([\n",
        "        os.path.join(dirpath, \"Articles*.csv\"),\n",
        "        os.path.join(dirpath, \"*articles*.csv\"),\n",
        "        os.path.join(dirpath, \"articles*.csv\"),\n",
        "    ])\n",
        "    if not a_paths:\n",
        "        raise FileNotFoundError(f\"No article CSVs in {dirpath}\")\n",
        "\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in a_paths], ignore_index=True)\n",
        "    arts = _normalize_articles_small(arts)\n",
        "\n",
        "    # Drop dups, keep earliest by pub_date\n",
        "    arts = arts.sort_values(\"pub_date\").drop_duplicates(subset=[\"article_id\",\"url\"], keep=\"first\")\n",
        "\n",
        "    cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "\n",
        "    # Build ID map with BOTH raw and str keys\n",
        "    by_id: Dict[object, Tuple] = {}\n",
        "    for aid, t in zip(arts[\"article_id\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(aid): continue\n",
        "        by_id[aid] = t\n",
        "    for aid, t in zip(arts[\"article_id\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(aid): continue\n",
        "        by_id[str(aid)] = t  # string key variant\n",
        "\n",
        "    # URL map (cleaned)\n",
        "    by_url: Dict[str, Tuple] = {}\n",
        "    for u, t in zip(arts[\"url\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(u): continue\n",
        "        by_url[str(u)] = t\n",
        "\n",
        "    return by_id, by_url\n",
        "\n",
        "def comment_files(dirpath: str) -> List[str]:\n",
        "    c_paths = _find([\n",
        "        os.path.join(dirpath, \"Comments*.csv\"),\n",
        "        os.path.join(dirpath, \"*comments*.csv\"),\n",
        "        os.path.join(dirpath, \"comments*.csv\"),\n",
        "    ])\n",
        "    if not c_paths:\n",
        "        raise FileNotFoundError(f\"No comment CSVs in {dirpath}\")\n",
        "    return c_paths\n",
        "\n",
        "# --------- streaming join/filter ----------\n",
        "def process_comment_stream(by_id, by_url, cpaths: List[str], out_filtered_csv: str,\n",
        "                           chunksize: int = 25_000):\n",
        "    os.makedirs(os.path.dirname(out_filtered_csv), exist_ok=True)\n",
        "    if os.path.exists(out_filtered_csv):\n",
        "        os.remove(out_filtered_csv)\n",
        "\n",
        "    # Only read the minimal set of columns we care about\n",
        "    allowed = set(sum(COM_MAP.values(), []))  # flatten list of lists\n",
        "    usecols = lambda c: c in allowed  # callable avoids errors if some don't exist\n",
        "\n",
        "    for p in cpaths:\n",
        "        print(f\"Processing: {os.path.basename(p)}\")\n",
        "        for i, raw in enumerate(pd.read_csv(p, low_memory=False, chunksize=chunksize, usecols=usecols)):\n",
        "            c = _normalize_comments_small(raw)\n",
        "\n",
        "            # Prepare empty article columns\n",
        "            for col in [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]:\n",
        "                c[col] = pd.NA\n",
        "\n",
        "            cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "\n",
        "            # --- ID lookups (try raw first, then str as fallback) ---\n",
        "            aid_raw = c[\"article_id\"]\n",
        "            got_raw = aid_raw.map(by_id)            # matches when types align\n",
        "            mask_raw = got_raw.notna()\n",
        "            id_hits_raw = int(mask_raw.sum())\n",
        "            if id_hits_raw:\n",
        "                vals = got_raw[mask_raw].tolist()\n",
        "                filled = pd.DataFrame(vals, index=c.index[mask_raw], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[mask_raw, col] = filled[col].values\n",
        "\n",
        "            mask_need_id = c[\"headline\"].isna()\n",
        "            got_str = aid_raw.astype(str).where(mask_need_id, None).map(by_id)\n",
        "            mask_str = got_str.notna()\n",
        "            id_hits_str = int(mask_str.sum())\n",
        "            if id_hits_str:\n",
        "                vals2 = got_str[mask_str].tolist()\n",
        "                filled2 = pd.DataFrame(vals2, index=c.index[mask_str], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[mask_str, col] = filled2[col].values\n",
        "\n",
        "            # --- URL fallback ---\n",
        "            mask_need_url = c[\"headline\"].isna()\n",
        "            url_clean = c.loc[mask_need_url, \"article_url\"].map(_clean_url)\n",
        "            got_url = url_clean.map(by_url)\n",
        "            mask_url = got_url.notna()\n",
        "            url_hits = int(mask_url.sum())\n",
        "            if url_hits:\n",
        "                vals3 = got_url[mask_url].tolist()\n",
        "                filled3 = pd.DataFrame(vals3, index=c.index[mask_need_url][mask_url], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[c.index[mask_need_url][mask_url], col] = filled3[col].values\n",
        "\n",
        "            # --- Filtering (pass both possible URL hints) ---\n",
        "            keep_mask = c.apply(\n",
        "                lambda r: _is_domestic_row(\n",
        "                    r[\"section\"], r[\"subsection\"], r[\"headline\"], r[\"abstract\"], r[\"keywords\"],\n",
        "                    url_hint=r.get(\"url\"), url_hint2=r.get(\"article_url\")\n",
        "                ),\n",
        "                axis=1\n",
        "            )\n",
        "            out = c.loc[keep_mask, OUT_COLS].copy()\n",
        "\n",
        "            if len(out):\n",
        "                out.to_csv(out_filtered_csv, mode=\"a\", header=not os.path.exists(out_filtered_csv), index=False)\n",
        "\n",
        "            print(f\"  chunk {i+1}: total={len(c):,}, id_hits={id_hits_raw + id_hits_str:,}, url_hits={url_hits:,}, kept={len(out):,}\")\n",
        "\n",
        "            # free memory\n",
        "            del raw, c, out, got_raw, got_str, got_url\n",
        "            gc.collect()\n",
        "\n",
        "def build_dataset_ultra(path_2017_2018: str, path_2020: str,\n",
        "                        out_dir=\"/content/nyt_outputs\", chunksize=25_000):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 2017/2018\n",
        "    by_id_A, by_url_A = load_articles_build_maps(path_2017_2018)\n",
        "    filtA = os.path.join(out_dir, \"domestic_politics_econ_2017_2018.csv\")\n",
        "    process_comment_stream(by_id_A, by_url_A, comment_files(path_2017_2018), filtA, chunksize=chunksize)\n",
        "\n",
        "    # 2020\n",
        "    by_id_B, by_url_B = load_articles_build_maps(path_2020)\n",
        "    filtB = os.path.join(out_dir, \"domestic_politics_econ_2020.csv\")\n",
        "    process_comment_stream(by_id_B, by_url_B, comment_files(path_2020), filtB, chunksize=chunksize)\n",
        "\n",
        "    # combine\n",
        "    combo = os.path.join(out_dir, \"domestic_politics_econ_filtered.csv\")\n",
        "    if os.path.exists(combo): os.remove(combo)\n",
        "    for f in [filtA, filtB]:\n",
        "        if os.path.exists(f):\n",
        "            pd.read_csv(f, low_memory=False).to_csv(combo, mode=\"a\", header=not os.path.exists(combo), index=False)\n",
        "\n",
        "    print(\"Done. Outputs in:\", out_dir)\n"
      ],
      "metadata": {
        "id": "yPZBZVP37xec"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path_17_18 = kagglehub.dataset_download(\"aashita/nyt-comments\")\n",
        "path_2020  = kagglehub.dataset_download(\"benjaminawd/new-york-times-articles-comments-2020\")\n",
        "\n",
        "build_dataset_ultra(\n",
        "    path_2017_2018=path_17_18,\n",
        "    path_2020=path_2020,\n",
        "    out_dir=\"/content/nyt_outputs\",\n",
        "    chunksize=25_000   # try 10_000 if memory is tight\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X761K7qQ73k_",
        "outputId": "59aa96c2-1408-45e8-bb07-35dd612ae643"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nyt-comments' dataset.\n",
            "Using Colab cache for faster access to the 'new-york-times-articles-comments-2020' dataset.\n",
            "Processing: CommentsApril2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=14,057\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=12,885\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=12,288\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=8,941\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=15,331\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=17,184\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=18,535\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=12,635\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=13,714\n",
            "  chunk 10: total=37,664, id_hits=18,832, url_hits=0, kept=9,000\n",
            "Processing: CommentsApril2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=16,110\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=14,603\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=19,113\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=19,076\n",
            "  chunk 5: total=50,000, id_hits=17,737, url_hits=0, kept=12,122\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=15,023\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=11,398\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=13,867\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=11,870\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=11,313\n",
            "  chunk 11: total=29,848, id_hits=14,924, url_hits=0, kept=8,634\n",
            "Processing: CommentsFeb2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=16,540\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=18,547\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=17,918\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=21,940\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=18,675\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=18,857\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=17,019\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=16,877\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=18,206\n",
            "  chunk 10: total=16,814, id_hits=8,407, url_hits=0, kept=5,509\n",
            "Processing: CommentsFeb2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=17,929\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=11,854\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=16,529\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=16,146\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=14,817\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=13,978\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=16,311\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=14,793\n",
            "  chunk 9: total=30,564, id_hits=15,282, url_hits=0, kept=7,618\n",
            "Processing: CommentsJan2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=16,740\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=19,095\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=14,954\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=14,074\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=16,515\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=18,676\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=18,060\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=16,090\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=16,294\n",
            "  chunk 10: total=12,898, id_hits=6,449, url_hits=0, kept=4,839\n",
            "Processing: CommentsJan2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=15,411\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=14,650\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=17,262\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=14,819\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=14,112\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=15,497\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=15,155\n",
            "  chunk 8: total=50,000, id_hits=3,707, url_hits=0, kept=1,858\n",
            "  chunk 9: total=6,398, id_hits=0, url_hits=0, kept=0\n",
            "Processing: CommentsMarch2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=17,745\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=18,800\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=16,731\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=18,122\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=17,848\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=18,219\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=16,308\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=17,164\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=15,133\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=17,066\n",
            "  chunk 11: total=21,934, id_hits=10,967, url_hits=0, kept=7,311\n",
            "Processing: CommentsMarch2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=16,557\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=15,478\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=15,506\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=15,962\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=16,403\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=17,432\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=13,721\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=14,450\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=14,383\n",
            "  chunk 10: total=43,830, id_hits=21,915, url_hits=0, kept=11,963\n",
            "Processing: CommentsMay2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=17,966\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=19,187\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=14,679\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=19,702\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=18,826\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=17,865\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=18,456\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=16,611\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=13,554\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=19,042\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=19,900\n",
            "  chunk 12: total=2,778, id_hits=1,389, url_hits=0, kept=1,338\n",
            "Processing: nyt-comments-2020.csv\n",
            "  chunk 1: total=25,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 2: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 3: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 4: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 5: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 6: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 7: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 8: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 9: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 10: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 11: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 12: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 13: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 14: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 15: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 16: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 17: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 18: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 19: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 20: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 21: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 22: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 23: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 24: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 25: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 26: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 27: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 28: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 29: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 30: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 31: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 32: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 33: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 34: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 35: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 36: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 37: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 38: total=50,000, id_hits=0, url_hits=0, kept=0\n",
            "  chunk 39: total=50,000, id_hits=0, url_hits=0, kept=0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-361032636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_2020\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benjaminawd/new-york-times-articles-comments-2020\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m build_dataset_ultra(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpath_2017_2018\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_17_18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath_2020\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2305167698.py\u001b[0m in \u001b[0;36mbuild_dataset_ultra\u001b[0;34m(path_2017_2018, path_2020, out_dir, chunksize)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mby_id_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_url_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_articles_build_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mfiltB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"domestic_politics_econ_2020.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mprocess_comment_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby_id_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_url_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# combine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2305167698.py\u001b[0m in \u001b[0;36mprocess_comment_stream\u001b[0;34m(by_id, by_url, cpaths, out_filtered_csv, chunksize)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# --- Filtering (pass both possible URL hints) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             keep_mask = c.apply(\n\u001b[0m\u001b[1;32m    262\u001b[0m                 lambda r: _is_domestic_row(\n\u001b[1;32m    263\u001b[0m                     \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subsection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10373\u001b[0m         )\n\u001b[0;32m> 10374\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10376\u001b[0m     def map(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2305167698.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    261\u001b[0m             keep_mask = c.apply(\n\u001b[1;32m    262\u001b[0m                 lambda r: _is_domestic_row(\n\u001b[0;32m--> 263\u001b[0;31m                     \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subsection\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headline\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0murl_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_hint2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article_url\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 ),\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mcheck_dict_or_set_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ultra-low-memory NYT election-only pipeline (Colab friendly) ---\n",
        "import os, re, glob, math, gc\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "# ------------------- Helpers -------------------\n",
        "def _clean_url(u):\n",
        "    if pd.isna(u): return u\n",
        "    return str(u).strip().split('?')[0].rstrip('/').lower()\n",
        "\n",
        "def _to_lower_str(val):\n",
        "    if val is None or val is pd.NA: return \"\"\n",
        "    try:\n",
        "        if isinstance(val, float) and math.isnan(val): return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    if hasattr(pd, \"isna\") and pd.isna(val): return \"\"\n",
        "    return str(val).strip().lower()\n",
        "\n",
        "# Output columns\n",
        "OUT_COLS = [\n",
        "    \"comment_id\",\"parent_id\",\"user\",\"comment\",\"recommendations\",\"reply_count\",\n",
        "    \"editors_selection\",\"date\",\n",
        "    \"article_id\",\"article_url\",\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\n",
        "    \"abstract\",\"news_desk\",\"type_of_material\",\"keywords\",\n",
        "]\n",
        "\n",
        "# ------------------- Column maps -------------------\n",
        "# Articles: prefer uniqueID\n",
        "ART_MAP = {\n",
        "    \"article_id\": [\"uniqueID\",\"uniqueid\",\"uniqueId\",\"UniqueID\",\"articleID\",\"asset_id\",\"assetId\",\"id\",\"article_id\"],\n",
        "    \"url\": [\"articleURL\",\"url\",\"web_url\",\"link\"],\n",
        "    \"pub_date\": [\"pub_date\",\"pubDate\",\"date\",\"published_date\",\"PublicationDate\"],\n",
        "    \"section\": [\"section\",\"section_name\",\"newsSection\",\"news_desk\"],\n",
        "    \"subsection\": [\"subsection\",\"subsection_name\",\"sub_section\"],\n",
        "    \"headline\": [\"headline\",\"title\",\"main_headline\",\"headline.main\",\"Title\"],\n",
        "    \"abstract\": [\"abstract\",\"snippet\",\"abstract_text\",\"lead_paragraph\",\"Summary\"],\n",
        "    \"news_desk\": [\"news_desk\",\"desk\"],\n",
        "    \"type_of_material\": [\"type_of_material\",\"type\",\"material_type_facet\"],\n",
        "    \"keywords\": [\"keywords\",\"descriptors\",\"subject\",\"keywords_list\"],\n",
        "}\n",
        "\n",
        "# Comments: prefer articleID\n",
        "COM_MAP = {\n",
        "    \"comment_id\": [\"commentID\",\"comment_id\",\"id\"],\n",
        "    \"parent_id\": [\"parentID\",\"parent_id\",\"inReplyTo\"],\n",
        "    \"user\": [\"userDisplayName\",\"user\",\"display_name\",\"author\"],\n",
        "    \"comment\": [\"commentBody\",\"comment\",\"text\",\"body\",\"content\"],\n",
        "    \"recommendations\": [\"recommendations\",\"recommendCount\",\"recommendedCount\",\"recommendationCount\"],\n",
        "    \"reply_count\": [\"replyCount\",\"replies\",\"numReplies\"],\n",
        "    \"editors_selection\": [\"editorsSelection\",\"editors_selection\"],\n",
        "    \"date\": [\"createDate\",\"date\",\"timestamp\",\"createdAt\"],\n",
        "    \"article_url\": [\"articleURL\",\"url\",\"web_url\",\"articleUrl\",\"story_url\",\"storyUrl\",\"article_link\",\"articleLink\",\"link\"],\n",
        "    \"article_id\": [\"articleID\",\"articleid\",\"asset_id\",\"assetId\",\"article_id\",\"story_id\",\"storyId\"],\n",
        "}\n",
        "\n",
        "# ------------------- Election filtering -------------------\n",
        "ELECTION_KEYWORD_PATTERNS = [\n",
        "    r\"\\belection(s)?\\b\", r\"\\bprimary\\b\", r\"\\bcaucus(es)?\\b\", r\"\\brunoff\\b\",\n",
        "    r\"\\bmidterm(s)?\\b\", r\"\\bgeneral election\\b\",\n",
        "    r\"\\bballot(s)?\\b\", r\"\\babsentee\\b\", r\"\\bmail[- ]in\\b\", r\"\\bearly voting\\b\",\n",
        "    r\"\\bvoter(s)?\\b\", r\"\\bvoter registration\\b\", r\"\\bregistered voter(s)?\\b\",\n",
        "    r\"\\bvoting\\b\", r\"\\bpoll(s|ing place)?\\b\", r\"\\bturnout\\b\",\n",
        "    r\"\\bcampaign(s|ing)?\\b\", r\"\\bcandidate(s)?\\b\",\n",
        "    r\"\\bpac(s)?\\b\", r\"\\bsuper pac(s)?\\b\", r\"\\bfec\\b\",\n",
        "    r\"\\bdebate(s)?\\b\", r\"\\bendorse(ment|ments)?\\b\",\n",
        "]\n",
        "ELECTION_REGEX = re.compile(\"|\".join(ELECTION_KEYWORD_PATTERNS), re.IGNORECASE)\n",
        "\n",
        "ELECTION_URL_SEGMENTS = [\n",
        "    \"/politics/elections\", \"/elections/\", \"/interactive/us/elections\",\n",
        "    \"/politics/\", \"/us/politics/\", \"/live/20\",\n",
        "    \"/news-event/2020-election\", \"/news-event/2022-midterms\", \"/news-event/2024-election\"\n",
        "]\n",
        "\n",
        "ELECTION_SECTIONS = {\"politics\"}\n",
        "ELECTION_SUBSTR = [\"election\", \"campaign\", \"midterm\", \"primary\", \"vote\"]\n",
        "\n",
        "def _is_us_election_row(section, subsection, headline, abstract, keywords, url_hint=None, url_hint2=None):\n",
        "    def url_says_election(u):\n",
        "        u = _to_lower_str(u)\n",
        "        if not u:\n",
        "            return False\n",
        "        if any(seg in u for seg in ELECTION_URL_SEGMENTS):\n",
        "            return True\n",
        "        if any(tok in u for tok in [\n",
        "            \"election\", \"primary\", \"caucus\", \"midterm\", \"ballot\", \"campaign\",\n",
        "            \"voting\", \"turnout\", \"polls\"\n",
        "        ]):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Section/subsection\n",
        "    sec = _to_lower_str(section)\n",
        "    if sec in ELECTION_SECTIONS:\n",
        "        return True\n",
        "    sub = _to_lower_str(subsection)\n",
        "    if any(x in sub for x in ELECTION_SUBSTR):\n",
        "        return True\n",
        "\n",
        "    # Keywords/headline/abstract\n",
        "    if isinstance(keywords, list):\n",
        "        kwtxt = \" \".join(_to_lower_str(k) for k in keywords if k is not None)\n",
        "    else:\n",
        "        kwtxt = _to_lower_str(keywords)\n",
        "    if ELECTION_REGEX.search(kwtxt or \"\"):\n",
        "        return True\n",
        "    if ELECTION_REGEX.search(_to_lower_str(headline) or \"\"):\n",
        "        return True\n",
        "    if ELECTION_REGEX.search(_to_lower_str(abstract) or \"\"):\n",
        "        return True\n",
        "\n",
        "    # URL fallback\n",
        "    if url_says_election(url_hint) or url_says_election(url_hint2):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# ------------------- Normalizers -------------------\n",
        "def _first_present(df, candidates, default=pd.NA):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    return pd.Series([default]*len(df))\n",
        "\n",
        "def _normalize_articles_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = pd.DataFrame({\n",
        "        \"article_id\": _first_present(df, ART_MAP[\"article_id\"]),\n",
        "        \"url\": _first_present(df, ART_MAP[\"url\"]).map(_clean_url),\n",
        "        \"pub_date\": _first_present(df, ART_MAP[\"pub_date\"]),\n",
        "        \"section\": _first_present(df, ART_MAP[\"section\"]),\n",
        "        \"subsection\": _first_present(df, ART_MAP[\"subsection\"]),\n",
        "        \"headline\": _first_present(df, ART_MAP[\"headline\"]),\n",
        "        \"abstract\": _first_present(df, ART_MAP[\"abstract\"]),\n",
        "        \"news_desk\": _first_present(df, ART_MAP[\"news_desk\"]),\n",
        "        \"type_of_material\": _first_present(df, ART_MAP[\"type_of_material\"]),\n",
        "        \"keywords\": _first_present(df, ART_MAP[\"keywords\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "def _normalize_comments_small(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def _first_present_local(cands, default=pd.NA):\n",
        "        for c in cands:\n",
        "            if c in df.columns:\n",
        "                return df[c]\n",
        "        return pd.Series([default]*len(df))\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"comment_id\": _first_present_local(COM_MAP[\"comment_id\"]),\n",
        "        \"parent_id\": _first_present_local(COM_MAP[\"parent_id\"]),\n",
        "        \"user\": _first_present_local(COM_MAP[\"user\"]),\n",
        "        \"comment\": _first_present_local(COM_MAP[\"comment\"]),\n",
        "        \"recommendations\": _first_present_local(COM_MAP[\"recommendations\"]),\n",
        "        \"reply_count\": _first_present_local(COM_MAP[\"reply_count\"]),\n",
        "        \"editors_selection\": _first_present_local(COM_MAP[\"editors_selection\"]),\n",
        "        \"date\": _first_present_local(COM_MAP[\"date\"]),\n",
        "        \"article_url\": _first_present_local(COM_MAP[\"article_url\"]).map(_clean_url),\n",
        "        \"article_id\": _first_present_local(COM_MAP[\"article_id\"]),\n",
        "    })\n",
        "    return out\n",
        "\n",
        "# ------------------- Loader functions -------------------\n",
        "def _find(paths_patterns: List[str]) -> List[str]:\n",
        "    s = set()\n",
        "    for pat in paths_patterns:\n",
        "        s.update(glob.glob(pat))\n",
        "    return sorted(s)\n",
        "\n",
        "def load_articles_build_maps(dirpath: str):\n",
        "    a_paths = _find([\n",
        "        os.path.join(dirpath, \"Articles*.csv\"),\n",
        "        os.path.join(dirpath, \"*articles*.csv\"),\n",
        "        os.path.join(dirpath, \"articles*.csv\"),\n",
        "        os.path.join(dirpath, \"nyt-articles-2020.csv\"),\n",
        "    ])\n",
        "    if not a_paths:\n",
        "        raise FileNotFoundError(f\"No article CSVs in {dirpath}\")\n",
        "\n",
        "    arts = pd.concat([pd.read_csv(p, low_memory=False) for p in a_paths], ignore_index=True)\n",
        "    arts = _normalize_articles_small(arts)\n",
        "\n",
        "    # Deduplicate\n",
        "    arts = arts.sort_values(\"pub_date\").drop_duplicates(subset=[\"article_id\",\"url\"], keep=\"first\")\n",
        "\n",
        "    cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "\n",
        "    by_id: Dict[object, Tuple] = {}\n",
        "    for aid, t in zip(arts[\"article_id\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(aid): continue\n",
        "        by_id[aid] = t\n",
        "    for aid, t in zip(arts[\"article_id\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(aid): continue\n",
        "        by_id[str(aid)] = t\n",
        "\n",
        "    by_url: Dict[str, Tuple] = {}\n",
        "    for u, t in zip(arts[\"url\"], arts[cols].itertuples(index=False, name=None)):\n",
        "        if pd.isna(u): continue\n",
        "        by_url[str(u)] = t\n",
        "\n",
        "    return by_id, by_url\n",
        "\n",
        "def comment_files(dirpath: str) -> List[str]:\n",
        "    c_paths = _find([\n",
        "        os.path.join(dirpath, \"Comments*.csv\"),\n",
        "        os.path.join(dirpath, \"*comments*.csv\"),\n",
        "        os.path.join(dirpath, \"comments*.csv\"),\n",
        "        os.path.join(dirpath, \"nyt-comments-2020*.csv\"),\n",
        "    ])\n",
        "    if not c_paths:\n",
        "        raise FileNotFoundError(f\"No comment CSVs in {dirpath}\")\n",
        "    return c_paths\n",
        "\n",
        "# ------------------- Streaming join & election filtering -------------------\n",
        "def process_comment_stream(by_id, by_url, cpaths: List[str], out_filtered_csv: str, chunksize: int = 25_000):\n",
        "    os.makedirs(os.path.dirname(out_filtered_csv), exist_ok=True)\n",
        "    if os.path.exists(out_filtered_csv):\n",
        "        os.remove(out_filtered_csv)\n",
        "\n",
        "    for p in cpaths:\n",
        "        print(f\"Processing: {os.path.basename(p)}\")\n",
        "        for i, raw in enumerate(pd.read_csv(p, low_memory=False, chunksize=chunksize)):\n",
        "            c = _normalize_comments_small(raw)\n",
        "\n",
        "            cols = [\"headline\",\"abstract\",\"pub_date\",\"section\",\"subsection\",\"news_desk\",\"type_of_material\",\"keywords\",\"url\"]\n",
        "            for col in cols:\n",
        "                c[col] = pd.NA\n",
        "\n",
        "            # --- ID join (raw, then str) ---\n",
        "            aid_raw = c[\"article_id\"]\n",
        "            got_raw = aid_raw.map(by_id)\n",
        "            mask_raw = got_raw.notna()\n",
        "            id_hits_raw = int(mask_raw.sum())\n",
        "            if id_hits_raw:\n",
        "                vals = got_raw[mask_raw].tolist()\n",
        "                filled = pd.DataFrame(vals, index=c.index[mask_raw], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[mask_raw, col] = filled[col].values\n",
        "\n",
        "            mask_need_id = c[\"headline\"].isna()\n",
        "            got_str = aid_raw.astype(str).where(mask_need_id, None).map(by_id)\n",
        "            mask_str = got_str.notna()\n",
        "            id_hits_str = int(mask_str.sum())\n",
        "            if id_hits_str:\n",
        "                vals2 = got_str[mask_str].tolist()\n",
        "                filled2 = pd.DataFrame(vals2, index=c.index[mask_str], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[mask_str, col] = filled2[col].values\n",
        "\n",
        "            # --- URL fallback ---\n",
        "            mask_need_url = c[\"headline\"].isna()\n",
        "            url_clean = c.loc[mask_need_url, \"article_url\"].map(_clean_url)\n",
        "            got_url = url_clean.map(by_url)\n",
        "            mask_url = got_url.notna()\n",
        "            url_hits = int(mask_url.sum())\n",
        "            if url_hits:\n",
        "                vals3 = got_url[mask_url].tolist()\n",
        "                filled3 = pd.DataFrame(vals3, index=c.index[mask_need_url][mask_url], columns=cols)\n",
        "                for col in cols:\n",
        "                    c.loc[c.index[mask_need_url][mask_url], col] = filled3[col].values\n",
        "\n",
        "            # --- Election-only filter ---\n",
        "            keep_mask = c.apply(\n",
        "                lambda r: _is_us_election_row(\n",
        "                    r[\"section\"], r[\"subsection\"], r[\"headline\"], r[\"abstract\"], r[\"keywords\"],\n",
        "                    url_hint=r.get(\"url\"), url_hint2=r.get(\"article_url\")\n",
        "                ),\n",
        "                axis=1\n",
        "            )\n",
        "            out = c.loc[keep_mask, OUT_COLS].copy()\n",
        "\n",
        "            if len(out):\n",
        "                out.to_csv(out_filtered_csv, mode=\"a\", header=not os.path.exists(out_filtered_csv), index=False)\n",
        "\n",
        "            print(f\"  chunk {i+1}: total={len(c):,}, id_hits={id_hits_raw + id_hits_str:,}, url_hits={url_hits:,}, kept={len(out):,}\")\n",
        "\n",
        "            del raw, c, out, got_raw, got_str, got_url\n",
        "            gc.collect()\n",
        "\n",
        "# ------------------- Master runner -------------------\n",
        "def build_dataset_ultra(path_2017_2018: str, path_2020: str, out_dir=\"/content/nyt_outputs\", chunksize=25_000):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # 2017/2018\n",
        "    by_id_A, by_url_A = load_articles_build_maps(path_2017_2018)\n",
        "    filtA = os.path.join(out_dir, \"us_elections_2017_2018.csv\")\n",
        "    process_comment_stream(by_id_A, by_url_A, comment_files(path_2017_2018), filtA, chunksize=chunksize)\n",
        "\n",
        "    # 2020\n",
        "    by_id_B, by_url_B = load_articles_build_maps(path_2020)\n",
        "    filtB = os.path.join(out_dir, \"us_elections_2020.csv\")\n",
        "    process_comment_stream(by_id_B, by_url_B, comment_files(path_2020), filtB, chunksize=chunksize)\n",
        "\n",
        "    # combine\n",
        "    combo = os.path.join(out_dir, \"us_elections_combined.csv\")\n",
        "    if os.path.exists(combo): os.remove(combo)\n",
        "    for f in [filtA, filtB]:\n",
        "        if os.path.exists(f):\n",
        "            pd.read_csv(f, low_memory=False).to_csv(combo, mode=\"a\", header=not os.path.exists(combo), index=False)\n",
        "\n",
        "    print(\"Done. Outputs in:\", out_dir)\n"
      ],
      "metadata": {
        "id": "vksT7PV--e_H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path_17_18 = kagglehub.dataset_download(\"aashita/nyt-comments\")\n",
        "path_2020  = kagglehub.dataset_download(\"benjaminawd/new-york-times-articles-comments-2020\")\n",
        "\n",
        "build_dataset_ultra(\n",
        "    path_2017_2018=path_17_18,\n",
        "    path_2020=path_2020,\n",
        "    out_dir=\"/content/nyt_outputs\",\n",
        "    chunksize=25_000   # try 10_000 if your runtime RAM is low\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qr3YA-Fi-hCd",
        "outputId": "13d938e9-9929-4411-eea9-f5968bf0bee7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nyt-comments' dataset.\n",
            "Using Colab cache for faster access to the 'new-york-times-articles-comments-2020' dataset.\n",
            "Processing: CommentsApril2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=4,244\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=6,716\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=2,114\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=1,513\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=2,671\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=2,140\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=5,981\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=749\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=3,639\n",
            "  chunk 10: total=37,664, id_hits=18,832, url_hits=0, kept=4,657\n",
            "Processing: CommentsApril2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=8,532\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=8,177\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=11,063\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=7,605\n",
            "  chunk 5: total=50,000, id_hits=17,737, url_hits=0, kept=4,715\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=2,585\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=1,788\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=4,470\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=4,947\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=3,904\n",
            "  chunk 11: total=29,848, id_hits=14,924, url_hits=0, kept=4,238\n",
            "Processing: CommentsFeb2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=734\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=1,100\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=1,435\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=4,881\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=3,086\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=299\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=7,561\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=3,800\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=2,294\n",
            "  chunk 10: total=16,814, id_hits=8,407, url_hits=0, kept=2,667\n",
            "Processing: CommentsFeb2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=14,739\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=5,315\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=3,527\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=5,287\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=5,319\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=7,629\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=3,610\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=4,439\n",
            "  chunk 9: total=30,564, id_hits=15,282, url_hits=0, kept=1,172\n",
            "Processing: CommentsJan2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=6,096\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=4,542\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=1,733\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=2,557\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=2,099\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=1,893\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=1,451\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=3,908\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=2,017\n",
            "  chunk 10: total=12,898, id_hits=6,449, url_hits=0, kept=491\n",
            "Processing: CommentsJan2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=5,848\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=7,330\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=3,750\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=3,199\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=3,240\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=5,791\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=6,089\n",
            "  chunk 8: total=50,000, id_hits=3,707, url_hits=0, kept=0\n",
            "  chunk 9: total=6,398, id_hits=0, url_hits=0, kept=0\n",
            "Processing: CommentsMarch2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=9,361\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=14,099\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=1,437\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=1,800\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=4,426\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=7,760\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=3,797\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=1,635\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=1,048\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=1,014\n",
            "  chunk 11: total=21,934, id_hits=10,967, url_hits=0, kept=4,277\n",
            "Processing: CommentsMarch2018.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=3,753\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=2,327\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=3,978\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=8,299\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=11,491\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=7,780\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=3,001\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=4,840\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=6,169\n",
            "  chunk 10: total=43,830, id_hits=21,915, url_hits=0, kept=4,474\n",
            "Processing: CommentsMay2017.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=5,544\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=6,859\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=8,313\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=8,800\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=951\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=2,371\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=5,582\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=7,674\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=3,975\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=4,263\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=4,012\n",
            "  chunk 12: total=2,778, id_hits=1,389, url_hits=0, kept=0\n",
            "Processing: nyt-comments-2020.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=8,191\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=1,818\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=6,550\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=2,652\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=3,252\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=3,953\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=8,380\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=11,157\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=8,814\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=4,690\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=11,021\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=8,664\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=14,417\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=9,182\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=6,276\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=6,169\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=11,608\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=17,023\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=12,131\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=13,244\n",
            "  chunk 21: total=50,000, id_hits=25,000, url_hits=0, kept=13,859\n",
            "  chunk 22: total=50,000, id_hits=25,000, url_hits=0, kept=11,856\n",
            "  chunk 23: total=50,000, id_hits=25,000, url_hits=0, kept=13,676\n",
            "  chunk 24: total=50,000, id_hits=25,000, url_hits=0, kept=11,817\n",
            "  chunk 25: total=50,000, id_hits=25,000, url_hits=0, kept=16,539\n",
            "  chunk 26: total=50,000, id_hits=25,000, url_hits=0, kept=8,648\n",
            "  chunk 27: total=50,000, id_hits=25,000, url_hits=0, kept=12,811\n",
            "  chunk 28: total=50,000, id_hits=25,000, url_hits=0, kept=16,937\n",
            "  chunk 29: total=50,000, id_hits=25,000, url_hits=0, kept=8,392\n",
            "  chunk 30: total=50,000, id_hits=25,000, url_hits=0, kept=19,348\n",
            "  chunk 31: total=50,000, id_hits=25,000, url_hits=0, kept=14,683\n",
            "  chunk 32: total=50,000, id_hits=25,000, url_hits=0, kept=15,819\n",
            "  chunk 33: total=50,000, id_hits=25,000, url_hits=0, kept=11,904\n",
            "  chunk 34: total=50,000, id_hits=25,000, url_hits=0, kept=10,656\n",
            "  chunk 35: total=50,000, id_hits=25,000, url_hits=0, kept=8,734\n",
            "  chunk 36: total=50,000, id_hits=25,000, url_hits=0, kept=11,075\n",
            "  chunk 37: total=50,000, id_hits=25,000, url_hits=0, kept=14,499\n",
            "  chunk 38: total=50,000, id_hits=25,000, url_hits=0, kept=15,401\n",
            "  chunk 39: total=50,000, id_hits=25,000, url_hits=0, kept=12,359\n",
            "  chunk 40: total=50,000, id_hits=25,000, url_hits=0, kept=12,536\n",
            "  chunk 41: total=50,000, id_hits=25,000, url_hits=0, kept=14,353\n",
            "  chunk 42: total=50,000, id_hits=25,000, url_hits=0, kept=8,771\n",
            "  chunk 43: total=50,000, id_hits=25,000, url_hits=0, kept=12,871\n",
            "  chunk 44: total=50,000, id_hits=25,000, url_hits=0, kept=5,777\n",
            "  chunk 45: total=50,000, id_hits=25,000, url_hits=0, kept=3,761\n",
            "  chunk 46: total=50,000, id_hits=25,000, url_hits=0, kept=8,100\n",
            "  chunk 47: total=50,000, id_hits=25,000, url_hits=0, kept=2,210\n",
            "  chunk 48: total=50,000, id_hits=25,000, url_hits=0, kept=4,334\n",
            "  chunk 49: total=50,000, id_hits=25,000, url_hits=0, kept=4,481\n",
            "  chunk 50: total=50,000, id_hits=25,000, url_hits=0, kept=2,478\n",
            "  chunk 51: total=50,000, id_hits=25,000, url_hits=0, kept=2,099\n",
            "  chunk 52: total=50,000, id_hits=25,000, url_hits=0, kept=2,620\n",
            "  chunk 53: total=50,000, id_hits=25,000, url_hits=0, kept=2,594\n",
            "  chunk 54: total=50,000, id_hits=25,000, url_hits=0, kept=1,808\n",
            "  chunk 55: total=50,000, id_hits=25,000, url_hits=0, kept=4,592\n",
            "  chunk 56: total=50,000, id_hits=25,000, url_hits=0, kept=3,483\n",
            "  chunk 57: total=50,000, id_hits=25,000, url_hits=0, kept=6,311\n",
            "  chunk 58: total=50,000, id_hits=25,000, url_hits=0, kept=2,471\n",
            "  chunk 59: total=50,000, id_hits=25,000, url_hits=0, kept=1,047\n",
            "  chunk 60: total=50,000, id_hits=25,000, url_hits=0, kept=2,715\n",
            "  chunk 61: total=50,000, id_hits=25,000, url_hits=0, kept=6,273\n",
            "  chunk 62: total=50,000, id_hits=25,000, url_hits=0, kept=9,557\n",
            "  chunk 63: total=50,000, id_hits=25,000, url_hits=0, kept=5,535\n",
            "  chunk 64: total=50,000, id_hits=25,000, url_hits=0, kept=5,862\n",
            "  chunk 65: total=50,000, id_hits=25,000, url_hits=0, kept=8,642\n",
            "  chunk 66: total=50,000, id_hits=25,000, url_hits=0, kept=5,825\n",
            "  chunk 67: total=50,000, id_hits=25,000, url_hits=0, kept=1,941\n",
            "  chunk 68: total=50,000, id_hits=25,000, url_hits=0, kept=2,310\n",
            "  chunk 69: total=50,000, id_hits=25,000, url_hits=0, kept=6,075\n",
            "  chunk 70: total=50,000, id_hits=25,000, url_hits=0, kept=2,861\n",
            "  chunk 71: total=50,000, id_hits=25,000, url_hits=0, kept=1,240\n",
            "  chunk 72: total=50,000, id_hits=25,000, url_hits=0, kept=2,503\n",
            "  chunk 73: total=50,000, id_hits=25,000, url_hits=0, kept=4,179\n",
            "  chunk 74: total=50,000, id_hits=25,000, url_hits=0, kept=5,684\n",
            "  chunk 75: total=50,000, id_hits=25,000, url_hits=0, kept=7,185\n",
            "  chunk 76: total=50,000, id_hits=25,000, url_hits=0, kept=7,054\n",
            "  chunk 77: total=50,000, id_hits=25,000, url_hits=0, kept=12,071\n",
            "  chunk 78: total=50,000, id_hits=25,000, url_hits=0, kept=5,805\n",
            "  chunk 79: total=50,000, id_hits=25,000, url_hits=0, kept=4,997\n",
            "  chunk 80: total=50,000, id_hits=25,000, url_hits=0, kept=7,974\n",
            "  chunk 81: total=50,000, id_hits=25,000, url_hits=0, kept=10,263\n",
            "  chunk 82: total=50,000, id_hits=25,000, url_hits=0, kept=1,695\n",
            "  chunk 83: total=50,000, id_hits=25,000, url_hits=0, kept=4,654\n",
            "  chunk 84: total=50,000, id_hits=25,000, url_hits=0, kept=5,485\n",
            "  chunk 85: total=50,000, id_hits=25,000, url_hits=0, kept=5,428\n",
            "  chunk 86: total=50,000, id_hits=25,000, url_hits=0, kept=1,689\n",
            "  chunk 87: total=50,000, id_hits=25,000, url_hits=0, kept=4,310\n",
            "  chunk 88: total=50,000, id_hits=25,000, url_hits=0, kept=5,261\n",
            "  chunk 89: total=50,000, id_hits=25,000, url_hits=0, kept=6,874\n",
            "  chunk 90: total=50,000, id_hits=25,000, url_hits=0, kept=1,805\n",
            "  chunk 91: total=50,000, id_hits=25,000, url_hits=0, kept=1,423\n",
            "  chunk 92: total=50,000, id_hits=25,000, url_hits=0, kept=3,465\n",
            "  chunk 93: total=50,000, id_hits=25,000, url_hits=0, kept=6,971\n",
            "  chunk 94: total=50,000, id_hits=25,000, url_hits=0, kept=3,082\n",
            "  chunk 95: total=50,000, id_hits=25,000, url_hits=0, kept=2,572\n",
            "  chunk 96: total=50,000, id_hits=25,000, url_hits=0, kept=2,851\n",
            "  chunk 97: total=50,000, id_hits=25,000, url_hits=0, kept=6,986\n",
            "  chunk 98: total=50,000, id_hits=25,000, url_hits=0, kept=5,384\n",
            "  chunk 99: total=50,000, id_hits=25,000, url_hits=0, kept=7,109\n",
            "  chunk 100: total=50,000, id_hits=25,000, url_hits=0, kept=6,448\n",
            "  chunk 101: total=50,000, id_hits=25,000, url_hits=0, kept=3,816\n",
            "  chunk 102: total=50,000, id_hits=25,000, url_hits=0, kept=1,617\n",
            "  chunk 103: total=50,000, id_hits=25,000, url_hits=0, kept=8,404\n",
            "  chunk 104: total=50,000, id_hits=25,000, url_hits=0, kept=15,048\n",
            "  chunk 105: total=50,000, id_hits=25,000, url_hits=0, kept=7,050\n",
            "  chunk 106: total=50,000, id_hits=25,000, url_hits=0, kept=7,026\n",
            "  chunk 107: total=50,000, id_hits=25,000, url_hits=0, kept=4,100\n",
            "  chunk 108: total=50,000, id_hits=25,000, url_hits=0, kept=8,125\n",
            "  chunk 109: total=50,000, id_hits=25,000, url_hits=0, kept=2,569\n",
            "  chunk 110: total=50,000, id_hits=25,000, url_hits=0, kept=8,578\n",
            "  chunk 111: total=50,000, id_hits=25,000, url_hits=0, kept=3,004\n",
            "  chunk 112: total=50,000, id_hits=25,000, url_hits=0, kept=7,444\n",
            "  chunk 113: total=50,000, id_hits=25,000, url_hits=0, kept=11,237\n",
            "  chunk 114: total=50,000, id_hits=25,000, url_hits=0, kept=1,650\n",
            "  chunk 115: total=50,000, id_hits=25,000, url_hits=0, kept=10,576\n",
            "  chunk 116: total=50,000, id_hits=25,000, url_hits=0, kept=8,064\n",
            "  chunk 117: total=50,000, id_hits=25,000, url_hits=0, kept=4,076\n",
            "  chunk 118: total=50,000, id_hits=25,000, url_hits=0, kept=6,599\n",
            "  chunk 119: total=50,000, id_hits=25,000, url_hits=0, kept=5,984\n",
            "  chunk 120: total=50,000, id_hits=25,000, url_hits=0, kept=3,240\n",
            "  chunk 121: total=50,000, id_hits=25,000, url_hits=0, kept=6,210\n",
            "  chunk 122: total=50,000, id_hits=25,000, url_hits=0, kept=4,377\n",
            "  chunk 123: total=50,000, id_hits=25,000, url_hits=0, kept=8,347\n",
            "  chunk 124: total=50,000, id_hits=25,000, url_hits=0, kept=9,109\n",
            "  chunk 125: total=50,000, id_hits=25,000, url_hits=0, kept=6,696\n",
            "  chunk 126: total=50,000, id_hits=25,000, url_hits=0, kept=10,757\n",
            "  chunk 127: total=50,000, id_hits=25,000, url_hits=0, kept=12,891\n",
            "  chunk 128: total=50,000, id_hits=25,000, url_hits=0, kept=9,725\n",
            "  chunk 129: total=50,000, id_hits=25,000, url_hits=0, kept=1,852\n",
            "  chunk 130: total=50,000, id_hits=25,000, url_hits=0, kept=8,557\n",
            "  chunk 131: total=50,000, id_hits=25,000, url_hits=0, kept=8,746\n",
            "  chunk 132: total=50,000, id_hits=25,000, url_hits=0, kept=15,500\n",
            "  chunk 133: total=50,000, id_hits=25,000, url_hits=0, kept=11,628\n",
            "  chunk 134: total=50,000, id_hits=25,000, url_hits=0, kept=12,909\n",
            "  chunk 135: total=50,000, id_hits=25,000, url_hits=0, kept=14,122\n",
            "  chunk 136: total=50,000, id_hits=25,000, url_hits=0, kept=14,592\n",
            "  chunk 137: total=50,000, id_hits=25,000, url_hits=0, kept=16,773\n",
            "  chunk 138: total=50,000, id_hits=25,000, url_hits=0, kept=12,861\n",
            "  chunk 139: total=50,000, id_hits=25,000, url_hits=0, kept=14,163\n",
            "  chunk 140: total=50,000, id_hits=25,000, url_hits=0, kept=14,555\n",
            "  chunk 141: total=50,000, id_hits=25,000, url_hits=0, kept=11,719\n",
            "  chunk 142: total=50,000, id_hits=25,000, url_hits=0, kept=13,607\n",
            "  chunk 143: total=50,000, id_hits=25,000, url_hits=0, kept=14,970\n",
            "  chunk 144: total=50,000, id_hits=25,000, url_hits=0, kept=5,073\n",
            "  chunk 145: total=50,000, id_hits=25,000, url_hits=0, kept=11,142\n",
            "  chunk 146: total=50,000, id_hits=25,000, url_hits=0, kept=8,056\n",
            "  chunk 147: total=50,000, id_hits=25,000, url_hits=0, kept=9,684\n",
            "  chunk 148: total=50,000, id_hits=25,000, url_hits=0, kept=11,209\n",
            "  chunk 149: total=50,000, id_hits=25,000, url_hits=0, kept=13,001\n",
            "  chunk 150: total=50,000, id_hits=25,000, url_hits=0, kept=11,388\n",
            "  chunk 151: total=50,000, id_hits=25,000, url_hits=0, kept=11,194\n",
            "  chunk 152: total=50,000, id_hits=25,000, url_hits=0, kept=5,992\n",
            "  chunk 153: total=50,000, id_hits=25,000, url_hits=0, kept=9,765\n",
            "  chunk 154: total=50,000, id_hits=25,000, url_hits=0, kept=13,559\n",
            "  chunk 155: total=50,000, id_hits=25,000, url_hits=0, kept=17,501\n",
            "  chunk 156: total=50,000, id_hits=25,000, url_hits=0, kept=16,076\n",
            "  chunk 157: total=50,000, id_hits=25,000, url_hits=0, kept=19,095\n",
            "  chunk 158: total=50,000, id_hits=25,000, url_hits=0, kept=15,408\n",
            "  chunk 159: total=50,000, id_hits=25,000, url_hits=0, kept=10,933\n",
            "  chunk 160: total=50,000, id_hits=25,000, url_hits=0, kept=12,513\n",
            "  chunk 161: total=50,000, id_hits=25,000, url_hits=0, kept=14,977\n",
            "  chunk 162: total=50,000, id_hits=25,000, url_hits=0, kept=11,757\n",
            "  chunk 163: total=50,000, id_hits=25,000, url_hits=0, kept=10,882\n",
            "  chunk 164: total=50,000, id_hits=25,000, url_hits=0, kept=10,976\n",
            "  chunk 165: total=50,000, id_hits=25,000, url_hits=0, kept=11,103\n",
            "  chunk 166: total=50,000, id_hits=25,000, url_hits=0, kept=12,555\n",
            "  chunk 167: total=50,000, id_hits=25,000, url_hits=0, kept=12,480\n",
            "  chunk 168: total=50,000, id_hits=25,000, url_hits=0, kept=18,896\n",
            "  chunk 169: total=50,000, id_hits=25,000, url_hits=0, kept=13,365\n",
            "  chunk 170: total=50,000, id_hits=25,000, url_hits=0, kept=15,281\n",
            "  chunk 171: total=50,000, id_hits=25,000, url_hits=0, kept=19,183\n",
            "  chunk 172: total=50,000, id_hits=25,000, url_hits=0, kept=15,185\n",
            "  chunk 173: total=50,000, id_hits=25,000, url_hits=0, kept=21,068\n",
            "  chunk 174: total=50,000, id_hits=25,000, url_hits=0, kept=21,158\n",
            "  chunk 175: total=50,000, id_hits=25,000, url_hits=0, kept=19,327\n",
            "  chunk 176: total=50,000, id_hits=25,000, url_hits=0, kept=17,518\n",
            "  chunk 177: total=50,000, id_hits=25,000, url_hits=0, kept=17,643\n",
            "  chunk 178: total=50,000, id_hits=25,000, url_hits=0, kept=15,183\n",
            "  chunk 179: total=50,000, id_hits=25,000, url_hits=0, kept=12,102\n",
            "  chunk 180: total=50,000, id_hits=25,000, url_hits=0, kept=12,437\n",
            "  chunk 181: total=50,000, id_hits=25,000, url_hits=0, kept=12,969\n",
            "  chunk 182: total=50,000, id_hits=25,000, url_hits=0, kept=9,204\n",
            "  chunk 183: total=50,000, id_hits=25,000, url_hits=0, kept=14,803\n",
            "  chunk 184: total=50,000, id_hits=25,000, url_hits=0, kept=11,392\n",
            "  chunk 185: total=50,000, id_hits=25,000, url_hits=0, kept=8,376\n",
            "  chunk 186: total=50,000, id_hits=25,000, url_hits=0, kept=6,683\n",
            "  chunk 187: total=50,000, id_hits=25,000, url_hits=0, kept=13,533\n",
            "  chunk 188: total=50,000, id_hits=25,000, url_hits=0, kept=7,423\n",
            "  chunk 189: total=50,000, id_hits=25,000, url_hits=0, kept=10,032\n",
            "  chunk 190: total=50,000, id_hits=25,000, url_hits=0, kept=10,826\n",
            "  chunk 191: total=50,000, id_hits=25,000, url_hits=0, kept=6,034\n",
            "  chunk 192: total=50,000, id_hits=25,000, url_hits=0, kept=16,783\n",
            "  chunk 193: total=50,000, id_hits=25,000, url_hits=0, kept=12,603\n",
            "  chunk 194: total=50,000, id_hits=25,000, url_hits=0, kept=8,953\n",
            "  chunk 195: total=50,000, id_hits=25,000, url_hits=0, kept=5,604\n",
            "  chunk 196: total=50,000, id_hits=25,000, url_hits=0, kept=3,907\n",
            "  chunk 197: total=50,000, id_hits=25,000, url_hits=0, kept=8,854\n",
            "  chunk 198: total=50,000, id_hits=25,000, url_hits=0, kept=4,661\n",
            "  chunk 199: total=50,000, id_hits=25,000, url_hits=0, kept=3,973\n",
            "  chunk 200: total=22,922, id_hits=11,461, url_hits=0, kept=1,554\n",
            "Processing: nyt-comments-part0.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=8,191\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=1,818\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=6,550\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=2,652\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=3,252\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=3,953\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=8,380\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=11,157\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=8,814\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=4,690\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=11,021\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=8,664\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=14,417\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=9,182\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=6,276\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=6,169\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=11,608\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=17,023\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=12,131\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=13,244\n",
            "Processing: nyt-comments-part1.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=13,859\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=11,856\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=13,676\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=11,817\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=16,539\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=8,648\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=12,811\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=16,937\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=8,392\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=19,348\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=14,683\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=15,819\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=11,904\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=10,656\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=8,734\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=11,075\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=14,499\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=15,401\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=12,359\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=12,536\n",
            "Processing: nyt-comments-part2.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=14,353\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=8,771\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=12,871\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=5,777\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=3,761\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=8,100\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=2,210\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=4,334\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=4,481\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=2,478\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=2,099\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=2,620\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=2,594\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=1,808\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=4,592\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=3,483\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=6,311\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=2,471\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=1,047\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=2,715\n",
            "Processing: nyt-comments-part3.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=6,273\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=9,557\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=5,535\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=5,862\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=8,642\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=5,825\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=1,941\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=2,310\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=6,075\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=2,861\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=1,240\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=2,503\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=4,179\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=5,684\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=7,185\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=7,054\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=12,071\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=5,805\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=4,997\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=7,974\n",
            "Processing: nyt-comments-part4.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=10,263\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=1,695\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=4,654\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=5,485\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=5,428\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=1,689\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=4,310\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=5,261\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=6,874\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=1,805\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=1,423\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=3,465\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=6,971\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=3,082\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=2,572\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=2,851\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=6,986\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=5,384\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=7,109\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=6,448\n",
            "Processing: nyt-comments-part5.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=3,816\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=1,617\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=8,404\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=15,048\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=7,050\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=7,026\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=4,100\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=8,125\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=2,569\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=8,578\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=3,004\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=7,444\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=11,237\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=1,650\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=10,576\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=8,064\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=4,076\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=6,599\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=5,984\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=3,240\n",
            "Processing: nyt-comments-part6.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=6,210\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=4,377\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=8,347\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=9,109\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=6,696\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=10,757\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=12,891\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=9,725\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=1,852\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=8,557\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=8,746\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=15,500\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=11,628\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=12,909\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=14,122\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=14,592\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=16,773\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=12,861\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=14,163\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=14,555\n",
            "Processing: nyt-comments-part7.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=11,719\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=13,607\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=14,970\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=5,073\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=11,142\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=8,056\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=9,684\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=11,209\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=13,001\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=11,388\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=11,194\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=5,992\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=9,765\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=13,559\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=17,501\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=16,076\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=19,095\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=15,408\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=10,933\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=12,513\n",
            "Processing: nyt-comments-part8.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=14,977\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=11,757\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=10,882\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=10,976\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=11,103\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=12,555\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=12,480\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=18,896\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=13,365\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=15,281\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=19,183\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=15,185\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=21,068\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=21,158\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=19,327\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=17,518\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=17,643\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=15,183\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=12,102\n",
            "  chunk 20: total=50,000, id_hits=25,000, url_hits=0, kept=12,437\n",
            "Processing: nyt-comments-part9.csv\n",
            "  chunk 1: total=25,000, id_hits=25,000, url_hits=0, kept=12,969\n",
            "  chunk 2: total=50,000, id_hits=25,000, url_hits=0, kept=9,204\n",
            "  chunk 3: total=50,000, id_hits=25,000, url_hits=0, kept=14,803\n",
            "  chunk 4: total=50,000, id_hits=25,000, url_hits=0, kept=11,392\n",
            "  chunk 5: total=50,000, id_hits=25,000, url_hits=0, kept=8,376\n",
            "  chunk 6: total=50,000, id_hits=25,000, url_hits=0, kept=6,683\n",
            "  chunk 7: total=50,000, id_hits=25,000, url_hits=0, kept=13,533\n",
            "  chunk 8: total=50,000, id_hits=25,000, url_hits=0, kept=7,423\n",
            "  chunk 9: total=50,000, id_hits=25,000, url_hits=0, kept=10,032\n",
            "  chunk 10: total=50,000, id_hits=25,000, url_hits=0, kept=10,826\n",
            "  chunk 11: total=50,000, id_hits=25,000, url_hits=0, kept=6,034\n",
            "  chunk 12: total=50,000, id_hits=25,000, url_hits=0, kept=16,783\n",
            "  chunk 13: total=50,000, id_hits=25,000, url_hits=0, kept=12,603\n",
            "  chunk 14: total=50,000, id_hits=25,000, url_hits=0, kept=8,953\n",
            "  chunk 15: total=50,000, id_hits=25,000, url_hits=0, kept=5,604\n",
            "  chunk 16: total=50,000, id_hits=25,000, url_hits=0, kept=3,907\n",
            "  chunk 17: total=50,000, id_hits=25,000, url_hits=0, kept=8,854\n",
            "  chunk 18: total=50,000, id_hits=25,000, url_hits=0, kept=4,661\n",
            "  chunk 19: total=50,000, id_hits=25,000, url_hits=0, kept=3,973\n",
            "  chunk 20: total=22,922, id_hits=11,461, url_hits=0, kept=1,554\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3755044320.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath_2020\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"benjaminawd/new-york-times-articles-comments-2020\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m build_dataset_ultra(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpath_2017_2018\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_17_18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath_2020\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_2020\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-591723978.py\u001b[0m in \u001b[0;36mbuild_dataset_ultra\u001b[0;34m(path_2017_2018, path_2020, out_dir, chunksize)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfiltA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done. Outputs in:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mwriters.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l /content/nyt_outputs/us_elections_2017_2018.csv\n",
        "!wc -l /content/nyt_outputs/us_elections_2020.csv\n",
        "!wc -l /content/nyt_outputs/us_elections_combined.csv\n",
        "!tail -n 3 /content/nyt_outputs/us_elections_2020.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQMMJRJxFP4e",
        "outputId": "9bd1606b-cc72-4960-eae7-68d00cf08c3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "402487 /content/nyt_outputs/us_elections_2017_2018.csv\n",
            "9898901 /content/nyt_outputs/us_elections_2020.csv\n",
            "10098834 /content/nyt_outputs/us_elections_combined.csv\n",
            "Fold itself in praying the Provider\n",
            "For Peace Food Love and Harmony\",0.0,0.0,True,2021-01-04 02:55:30,nyt://interactive/61cd3792-6672-5c18-9377-6cb118b4a125,,,2020-12-31 10:01:02+00:00,The Upshot,,How We Got Through and What We Missed Most: Lessons From a Pandemic Year,\"Readers across the country told us about their lockdown life. Baking, bourbon and biting nails? Check. But also, resilience and hope.\",,,\"['Quarantine (Life and Culture)', 'Coronavirus (2019-nCoV)', 'Polls and Public Opinion', 'Anxiety and Stress']\"\n",
            "110887111.0,,Nancy,\"I learned that the two of us only use three rolls of toilet paper a month, and if i go along with the panic and buy 24 rolls that shred when wet, i will be regretting it for a while.\",1.0,0.0,False,2021-01-04 18:15:52,nyt://interactive/61cd3792-6672-5c18-9377-6cb118b4a125,,,2020-12-31 10:01:02+00:00,The Upshot,,How We Got Through and What We Missed Most: Lessons From a Pandemic Year,\"Readers across the country told us about their lockdown life. Baking, bourbon and biting nails? Check. But also, resilience and hope.\",,,\"['Quarantine (Life and Culture)', 'Coronavirus (2019-nCoV)', 'Polls and Public Opinion', 'Anxiety and Stress']\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re, math, os\n",
        "\n",
        "in_dir  = \"/content/nyt_outputs\"\n",
        "files_in = [\n",
        "    os.path.join(in_dir, \"us_elections_2017_2018.csv\"),\n",
        "    os.path.join(in_dir, \"us_elections_2020.csv\"),\n",
        "    os.path.join(in_dir, \"us_elections_combined.csv\"),\n",
        "]\n",
        "\n",
        "def _to_lower_str(v):\n",
        "    if v is None: return \"\"\n",
        "    try:\n",
        "        if v is pd.NA or (isinstance(v, float) and math.isnan(v)): return \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    if pd.isna(v): return \"\"\n",
        "    return str(v).strip().lower()\n",
        "\n",
        "# Strict, election-only signals (no generic â€œpollsâ€ on its own)\n",
        "CORE_ELECTION = re.compile(r\"\"\"\n",
        "    \\b(election|primary|caucus|runoff|midterm|ballot|absentee|mail[- ]in|\n",
        "       early\\ voting|voter|voting|turnout|campaign|candidate|fec|debate|endorsement)s?\\b\n",
        "\"\"\", re.IGNORECASE | re.VERBOSE)\n",
        "\n",
        "URL_ELECTION_SEGMENTS = (\n",
        "    \"/elections/\", \"/politics/elections\", \"/us/politics/elections\",\n",
        "    \"/news-event/2020-election\", \"/news-event/2022-midterms\", \"/news-event/2024-election\"\n",
        ")\n",
        "\n",
        "SECTIONS_ALLOWED = {\"politics\"}  # (You can add \"U.S.\" if you want broader scope)\n",
        "SUBSTR_ALLOWED   = (\"election\",\"campaign\",\"midterm\",\"primary\",\"vote\")\n",
        "\n",
        "# Exclude clearly non-election pandemic pieces *unless* they also match CORE_ELECTION\n",
        "EXCLUDE_PANDEMIC = re.compile(r\"\\b(covid|coronavirus|pandemic|quarantine|lockdown)\\b\", re.IGNORECASE)\n",
        "\n",
        "def looks_like_election_row(row):\n",
        "    section    = _to_lower_str(row.get(\"section\"))\n",
        "    subsection = _to_lower_str(row.get(\"subsection\"))\n",
        "    headline   = _to_lower_str(row.get(\"headline\"))\n",
        "    abstract   = _to_lower_str(row.get(\"abstract\"))\n",
        "    keywords   = row.get(\"keywords\")\n",
        "    url1       = _to_lower_str(row.get(\"url\"))\n",
        "    url2       = _to_lower_str(row.get(\"article_url\"))\n",
        "\n",
        "    # Section/subsection cues\n",
        "    if section in SECTIONS_ALLOWED: return True\n",
        "    if any(k in subsection for k in SUBSTR_ALLOWED): return True\n",
        "\n",
        "    # URL cues (explicit election paths)\n",
        "    if any(seg in url1 for seg in URL_ELECTION_SEGMENTS): return True\n",
        "    if any(seg in url2 for seg in URL_ELECTION_SEGMENTS): return True\n",
        "\n",
        "    # Keywords / text cues\n",
        "    kwtxt = \"\"\n",
        "    if isinstance(keywords, list):\n",
        "        kwtxt = \" \".join(_to_lower_str(k) for k in keywords if k is not None)\n",
        "    else:\n",
        "        kwtxt = _to_lower_str(keywords)\n",
        "\n",
        "    strong_text_hit = bool(\n",
        "        CORE_ELECTION.search(kwtxt) or CORE_ELECTION.search(headline) or CORE_ELECTION.search(abstract)\n",
        "    )\n",
        "\n",
        "    # If pandemic-y but no strong election terms â†’ exclude\n",
        "    if EXCLUDE_PANDEMIC.search(\" \".join([headline, abstract, kwtxt])) and not strong_text_hit:\n",
        "        return False\n",
        "\n",
        "    return strong_text_hit\n",
        "\n",
        "def refilter_file(path_in):\n",
        "    path_out = path_in.replace(\".csv\", \"_strict.csv\")\n",
        "    if os.path.exists(path_out):\n",
        "        os.remove(path_out)\n",
        "\n",
        "    kept = 0\n",
        "    total = 0\n",
        "    for chunk in pd.read_csv(path_in, low_memory=False, chunksize=200_000):\n",
        "        mask = chunk.apply(looks_like_election_row, axis=1)\n",
        "        out  = chunk.loc[mask]\n",
        "        total += len(chunk)\n",
        "        kept  += len(out)\n",
        "        out.to_csv(path_out, mode=\"a\", header=not os.path.exists(path_out), index=False)\n",
        "        print(f\"{os.path.basename(path_in)}: processed={total:,}, kept={kept:,}\")\n",
        "    return path_out, kept, total\n",
        "\n",
        "strict_paths = []\n",
        "for f in files_in:\n",
        "    if os.path.exists(f):\n",
        "        strict_paths.append(refilter_file(f)[0])\n",
        "\n",
        "print(\"\\nStrict files written:\")\n",
        "for p in strict_paths:\n",
        "    print(\" -\", p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "Ozc-2sobFwJB",
        "outputId": "4b1ae026-222f-456e-9249-2dba4b02fcc7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "us_elections_2017_2018.csv: processed=200,000, kept=194,662\n",
            "us_elections_2017_2018.csv: processed=400,000, kept=387,492\n",
            "us_elections_2017_2018.csv: processed=402,486, kept=389,978\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1649933296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mstrict_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefilter_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStrict files written:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1649933296.py\u001b[0m in \u001b[0;36mrefilter_file\u001b[0;34m(path_in)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mkept\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{os.path.basename(path_in)}: processed={total:,}, kept={kept:,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mwriters.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/nyt_outputs\n",
        "!cp -f /content/nyt_outputs/*.csv /content/drive/MyDrive/nyt_outputs/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XngchDGoGC0v",
        "outputId": "73d1f5f1-dad6-4bcc-f27a-f28dca151ca2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -f /content/nyt_outputs/us_elections_2017_2018.csv /content/drive/MyDrive/nyt_outputs/\n",
        "!cp -f /content/nyt_outputs/us_elections_combined.csv /content/drive/MyDrive/nyt_outputs/\n"
      ],
      "metadata": {
        "id": "6dc5Tp4_Gkji"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/drive/MyDrive/nyt_outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg1phT6SG_KA",
        "outputId": "fb49ff7f-8882-4296-9bc4-895e651a35f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.9G\n",
            "-rw------- 1 root root 1006M Oct 21 18:47 domestic_politics_econ_2017_2018.csv\n",
            "-rw------- 1 root root  306M Oct 21 18:49 us_elections_2017_2018.csv\n",
            "-rw------- 1 root root  297M Oct 21 18:47 us_elections_2017_2018_strict.csv\n",
            "-rw------- 1 root root  3.2G Oct 21 18:47 us_elections_2020.csv\n",
            "-rw------- 1 root root  121M Oct 21 18:47 us_elections_2020_strict.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -f /content/nyt_outputs/us_elections_combined.csv /content/drive/MyDrive/nyt_outputs/\n"
      ],
      "metadata": {
        "id": "W_qR7jQ7HpM5"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}