{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TsRyVD7rAQ6",
        "outputId": "9029295b-4c22-42cb-b7e4-b6a9ee9b1c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbIJUWEHqMdh",
        "outputId": "db597388-aa92-44be-d13c-d74fa1996ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: us_elections_2017_2018_strict.csv\n",
            "Processing: us_elections_2020_strict.csv\n",
            "Finalizing…\n",
            "Wrote -> /content/drive/MyDrive/nyt_outputs/article_level_targets_textblob.csv rows: 950\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install textblob\n",
        "\n",
        "import os, math, gc\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from textblob import TextBlob\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/nyt_outputs\"\n",
        "COMBINED_STRICT = os.path.join(DRIVE_DIR, \"us_elections_combined_strict.csv\")\n",
        "STRICT_1718    = os.path.join(DRIVE_DIR, \"us_elections_2017_2018_strict.csv\")\n",
        "STRICT_2020    = os.path.join(DRIVE_DIR, \"us_elections_2020_strict.csv\")\n",
        "OUT_CSV        = os.path.join(DRIVE_DIR, \"article_level_targets_textblob.csv\")\n",
        "\n",
        "\n",
        "INPUTS = [COMBINED_STRICT] if os.path.exists(COMBINED_STRICT) else [p for p in (STRICT_1718, STRICT_2020) if os.path.exists(p)]\n",
        "assert INPUTS, \"No strict CSVs found in Drive. Copy *_strict.csv files to /content/drive/MyDrive/nyt_outputs first.\"\n",
        "\n",
        "\n",
        "META_COLS = [\"url\",\"pub_date\",\"section\",\"subsection\",\"headline\",\"abstract\",\"news_desk\",\"type_of_material\",\"keywords\"]\n",
        "NEEDED_COLS = [\"article_id\",\"comment\"] + META_COLS\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def safe_polarity(text):\n",
        "    if text is None or (isinstance(text, float) and math.isnan(text)):\n",
        "        return 0.0\n",
        "    s = str(text).strip()\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    try:\n",
        "        return TextBlob(s).sentiment.polarity  # [-1,1]\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Aggregation holders\n",
        "# numeric: sum of polarity and count of comments\n",
        "agg_sum = defaultdict(float)   # article_id -> sum polarity\n",
        "agg_cnt = defaultdict(int)     # article_id -> num comments\n",
        "# metadata\n",
        "agg_meta = {}  # article_id -> dict of META_COLS\n",
        "\n",
        "# ---------- main loop (chunked) ----------\n",
        "for path in INPUTS:\n",
        "    print(f\"Processing: {os.path.basename(path)}\")\n",
        "    for i, chunk in enumerate(pd.read_csv(path, low_memory=False, chunksize=100_000, usecols=lambda c: c in NEEDED_COLS, dtype={\"article_id\": str})):\n",
        "        # sentiment per comment\n",
        "        chunk[\"comment\"] = chunk[\"comment\"].astype(str)\n",
        "        chunk[\"polarity\"] = chunk[\"comment\"].map(safe_polarity)\n",
        "\n",
        "        # numeric aggregation per chunk\n",
        "        sums  = chunk.groupby(\"article_id\")[\"polarity\"].sum()\n",
        "        counts= chunk.groupby(\"article_id\")[\"polarity\"].count()\n",
        "\n",
        "        for aid, s in sums.items():\n",
        "            agg_sum[aid] += float(s)\n",
        "        for aid, n in counts.items():\n",
        "            agg_cnt[aid] += int(n)\n",
        "\n",
        "        # metadata: take first occurrence per article_id if not already stored\n",
        "        meta_first = chunk.drop_duplicates(\"article_id\")[[\"article_id\"] + META_COLS].set_index(\"article_id\")\n",
        "        for aid, row in meta_first.iterrows():\n",
        "            if aid not in agg_meta:\n",
        "                agg_meta[aid] = {k: row.get(k) for k in META_COLS}\n",
        "\n",
        "        if (i+1) % 20 == 0:\n",
        "            print(f\"  chunks processed: {i+1}, distinct articles so far: {len(agg_meta):,}\")\n",
        "        del chunk, sums, counts, meta_first\n",
        "        gc.collect()\n",
        "\n",
        "print(\"Finalizing…\")\n",
        "\n",
        "# ---------- build output frame ----------\n",
        "rows = []\n",
        "for aid in agg_cnt.keys():\n",
        "    total = agg_cnt[aid]\n",
        "    sent_mean = (agg_sum[aid] / total) if total else 0.0\n",
        "    meta = agg_meta.get(aid, {})\n",
        "    rows.append({\n",
        "        \"article_id\": aid,\n",
        "        **{k: meta.get(k) for k in META_COLS},\n",
        "        \"comments_total\": total,\n",
        "        \"sentiment_mean_textblob\": sent_mean,\n",
        "    })\n",
        "\n",
        "out_df = pd.DataFrame(rows)\n",
        "\n",
        "#sort by pub_date then comments_total desc\n",
        "if \"pub_date\" in out_df.columns:\n",
        "    out_df = out_df.sort_values(by=[\"pub_date\",\"comments_total\"], ascending=[True, False], ignore_index=True)\n",
        "\n",
        "# Write to Drive\n",
        "out_df.to_csv(OUT_CSV, index=False)\n",
        "print(\"Wrote ->\", OUT_CSV, \"rows:\", len(out_df))\n"
      ]
    }
  ]
}